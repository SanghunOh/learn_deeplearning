{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T13:22:06.438117Z","iopub.execute_input":"2024-10-31T13:22:06.438858Z","iopub.status.idle":"2024-10-31T13:22:07.546408Z","shell.execute_reply.started":"2024-10-31T13:22:06.438788Z","shell.execute_reply":"2024-10-31T13:22:07.545366Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Tensor Operations","metadata":{}},{"cell_type":"code","source":"import torch\n# 2x3x4 텐서 생성\nx = torch.randn(2, 3, 4)\nprint(x)\nprint(\"\\nOriginal Tensor (2x3x4):\")\nprint(x.size())\nprint()\n### 기본적인 슬라이싱 ### #첫 번째 슬라이스 (0번째 텐서 선택)\nx_slice_0 = x[0]\nprint(x_slice_0)\nprint(\"\\nSlice along first dimension (x[0]):\")\nprint(x_slice_0.size())\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:23:28.565580Z","iopub.execute_input":"2024-10-31T13:23:28.566478Z","iopub.status.idle":"2024-10-31T13:23:30.510245Z","shell.execute_reply.started":"2024-10-31T13:23:28.566433Z","shell.execute_reply":"2024-10-31T13:23:30.509244Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensor([[[-2.1317, -0.2064,  1.1999,  0.6580],\n         [ 0.2599, -0.6291, -0.1057, -1.8998],\n         [ 0.1133,  0.4788,  1.0202, -1.3403]],\n\n        [[-0.1174,  0.9258, -1.6861,  1.1877],\n         [ 0.7424,  1.0666,  0.0659,  0.3272],\n         [-1.4311,  0.0354, -2.2960,  0.0814]]])\n\nOriginal Tensor (2x3x4):\ntorch.Size([2, 3, 4])\n\ntensor([[-2.1317, -0.2064,  1.1999,  0.6580],\n        [ 0.2599, -0.6291, -0.1057, -1.8998],\n        [ 0.1133,  0.4788,  1.0202, -1.3403]])\n\nSlice along first dimension (x[0]):\ntorch.Size([3, 4])\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# 모든 텐서에서 첫 번째 열 선택 (모든 첫 번째 차원, 첫 번째 열)\nx_slice_col_0 = x[:, 0]\nprint(x_slice_col_0)\nprint(\"\\nSlice along second dimension (x[:, 0]):\")\nprint(x_slice_col_0.size())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:24:46.611257Z","iopub.execute_input":"2024-10-31T13:24:46.611830Z","iopub.status.idle":"2024-10-31T13:24:46.626761Z","shell.execute_reply.started":"2024-10-31T13:24:46.611778Z","shell.execute_reply":"2024-10-31T13:24:46.625624Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tensor([[-2.1317, -0.2064,  1.1999,  0.6580],\n        [-0.1174,  0.9258, -1.6861,  1.1877]])\n\nSlice along second dimension (x[:, 0]):\ntorch.Size([2, 4])\n","output_type":"stream"}]},{"cell_type":"code","source":"### 마스크를 이용한 슬라이싱 ###\n#0보다 큰 원소들만 선택 (마스크 생성)\nmask = x > 0\nx_masked = x[mask]\nprint(mask)\nprint(\"\\Mask for selecting elements greater than 0:\")\nprint()\nprint()\nprint (x_masked)\nprint(\"\\nTensor with elements greater than 0 (x[mask]):\")\nprint(x_masked.size())\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:25:15.101176Z","iopub.execute_input":"2024-10-31T13:25:15.101557Z","iopub.status.idle":"2024-10-31T13:25:15.110982Z","shell.execute_reply.started":"2024-10-31T13:25:15.101522Z","shell.execute_reply":"2024-10-31T13:25:15.109976Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[[False, False,  True,  True],\n         [ True, False, False, False],\n         [ True,  True,  True, False]],\n\n        [[False,  True, False,  True],\n         [ True,  True,  True,  True],\n         [False,  True, False,  True]]])\n\\Mask for selecting elements greater than 0:\n\n\ntensor([1.1999, 0.6580, 0.2599, 0.1133, 0.4788, 1.0202, 0.9258, 1.1877, 0.7424,\n        1.0666, 0.0659, 0.3272, 0.0354, 0.0814])\n\nTensor with elements greater than 0 (x[mask]):\ntorch.Size([14])\n\n","output_type":"stream"}]},{"cell_type":"code","source":"###인덱스를 이용한 슬라이싱 ### # 0번째 차원의 특정 인덱스 선택\nindices = torch. tensor([0, 1])\nx_indices = x[indices]\nprint(x_indices)\nprint(\"\\nTensor with selected indices along first dimension (x[indices]):\")\nprint(x_indices.size())\nprint()\n# 인덱스를 사용하여 값 할당 (특정 인덱스에 값 할당)\nx[indices] = -5\nprint (x)\nprint(\"\\nTensor after assigning -5 to elements at indices (x[indices] = -5):\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:26:28.536481Z","iopub.execute_input":"2024-10-31T13:26:28.536878Z","iopub.status.idle":"2024-10-31T13:26:28.547651Z","shell.execute_reply.started":"2024-10-31T13:26:28.536839Z","shell.execute_reply":"2024-10-31T13:26:28.546725Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tensor([[[-2.1317, -0.2064,  1.1999,  0.6580],\n         [ 0.2599, -0.6291, -0.1057, -1.8998],\n         [ 0.1133,  0.4788,  1.0202, -1.3403]],\n\n        [[-0.1174,  0.9258, -1.6861,  1.1877],\n         [ 0.7424,  1.0666,  0.0659,  0.3272],\n         [-1.4311,  0.0354, -2.2960,  0.0814]]])\n\nTensor with selected indices along first dimension (x[indices]):\ntorch.Size([2, 3, 4])\n\ntensor([[[-5., -5., -5., -5.],\n         [-5., -5., -5., -5.],\n         [-5., -5., -5., -5.]],\n\n        [[-5., -5., -5., -5.],\n         [-5., -5., -5., -5.],\n         [-5., -5., -5., -5.]]])\n\nTensor after assigning -5 to elements at indices (x[indices] = -5):\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2x3x4 텐서 생성\nx = torch. randn(2, 3, 4)\nprint(\"Original Tensor:\")\nprint (x)\n# flatten 연산으로 1D 벡터로 변환\nx_flatten = x. flatten()\nprint(\" \\nFlattened Tensor:\")\nprint(x_flatten)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:37:53.727203Z","iopub.execute_input":"2024-10-31T13:37:53.728000Z","iopub.status.idle":"2024-10-31T13:37:53.736262Z","shell.execute_reply.started":"2024-10-31T13:37:53.727959Z","shell.execute_reply":"2024-10-31T13:37:53.735290Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Original Tensor:\ntensor([[[-1.9535, -0.1674,  1.0051, -0.1373],\n         [ 2.2412, -1.4994, -0.5365,  2.0509],\n         [ 1.8128, -0.8210, -1.2549, -0.9295]],\n\n        [[ 0.8497,  1.0882,  0.3300, -0.4107],\n         [ 1.0397, -1.2370,  1.0747,  0.5799],\n         [-0.7325, -1.6841,  0.0920,  1.4640]]])\n \nFlattened Tensor:\ntensor([-1.9535, -0.1674,  1.0051, -0.1373,  2.2412, -1.4994, -0.5365,  2.0509,\n         1.8128, -0.8210, -1.2549, -0.9295,  0.8497,  1.0882,  0.3300, -0.4107,\n         1.0397, -1.2370,  1.0747,  0.5799, -0.7325, -1.6841,  0.0920,  1.4640])\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1x3x1 텐서 생성\nx = torch. rand(1, 3, 1)\nprint(x)\nprint(\"\\nOriginal Tensor with size (1, 3, 1):\")\nprint(x.size())\n\nprint()\n# squeeze로 크기가 1인 차원 제거\nx_squeezed = x. squeeze()\nprint(x_squeezed)\nprint(\"\\nSqueezed Tensor (size reduced):\")\nprint(x_squeezed.size())\nprint()\nprint()\n# unsqueeze로 0번째 차원에 1을 추가\nx_unsqueezed = x_squeezed. unsqueeze (0)\nprint(x_unsqueezed)\nprint(\"\\nUnsqueezed Tensor (1 added to 0th dimension):\")\nprint(x_unsqueezed.size())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:48:44.901921Z","iopub.execute_input":"2024-10-31T13:48:44.902827Z","iopub.status.idle":"2024-10-31T13:48:44.913427Z","shell.execute_reply.started":"2024-10-31T13:48:44.902758Z","shell.execute_reply":"2024-10-31T13:48:44.912417Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([[[0.2741],\n         [0.1965],\n         [0.4995]]])\n\nOriginal Tensor with size (1, 3, 1):\ntorch.Size([1, 3, 1])\n\ntensor([0.2741, 0.1965, 0.4995])\n\nSqueezed Tensor (size reduced):\ntorch.Size([3])\n\n\ntensor([[0.2741, 0.1965, 0.4995]])\n\nUnsqueezed Tensor (1 added to 0th dimension):\ntorch.Size([1, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2X3x4 텐서 생성\nx = torch.randn(2, 3, 4)\nprint (x)\nprint(\"\\nOriginal Tensor (2x3x4):\")\nprint(x.size)\nprint()\n# permute로 차원의 순서 바꾸기\nx_permuted = x.permute(2, 0, 1)\nprint(x_permuted)\nprint(\"\\nPermuted Tensor (4x2x3):\")\nprint(x_permuted.size())\nprint()\n# transpose로 1번째와 2번째 차원 교환\nx_transposed = x.transpose (1, 2)\nprint(x_transposed)\nprint(\"\\nTransposed Tensor (2x4x3):\")\nprint(x_transposed.size)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:50:01.301076Z","iopub.execute_input":"2024-10-31T13:50:01.302126Z","iopub.status.idle":"2024-10-31T13:50:01.315887Z","shell.execute_reply.started":"2024-10-31T13:50:01.302072Z","shell.execute_reply":"2024-10-31T13:50:01.314748Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"tensor([[[-0.3426,  0.2142, -1.5337,  0.1134],\n         [ 0.0238, -1.6555, -1.3035, -0.4532],\n         [ 0.7210,  1.3330, -0.1822, -0.1738]],\n\n        [[-0.2393, -0.2465,  0.6416, -0.3948],\n         [-0.4242, -0.3488, -0.9103,  2.0453],\n         [ 0.8336,  0.3592, -1.6982,  0.3450]]])\n\nOriginal Tensor (2x3x4):\n<built-in method size of Tensor object at 0x7889b5ef9d00>\n\ntensor([[[-0.3426,  0.0238,  0.7210],\n         [-0.2393, -0.4242,  0.8336]],\n\n        [[ 0.2142, -1.6555,  1.3330],\n         [-0.2465, -0.3488,  0.3592]],\n\n        [[-1.5337, -1.3035, -0.1822],\n         [ 0.6416, -0.9103, -1.6982]],\n\n        [[ 0.1134, -0.4532, -0.1738],\n         [-0.3948,  2.0453,  0.3450]]])\n\nPermuted Tensor (4x2x3):\ntorch.Size([4, 2, 3])\n\ntensor([[[-0.3426,  0.0238,  0.7210],\n         [ 0.2142, -1.6555,  1.3330],\n         [-1.5337, -1.3035, -0.1822],\n         [ 0.1134, -0.4532, -0.1738]],\n\n        [[-0.2393, -0.4242,  0.8336],\n         [-0.2465, -0.3488,  0.3592],\n         [ 0.6416, -0.9103, -1.6982],\n         [-0.3948,  2.0453,  0.3450]]])\n\nTransposed Tensor (2x4x3):\n<built-in method size of Tensor object at 0x788a543f4e50>\n","output_type":"stream"}]},{"cell_type":"code","source":"# 3X3 텐서 2개 생성\nx1 = torch. randn(3, 3)\nx2 = torch. randn(3, 3)\nprint(\"\\nTwo 3x3 Tensors:\")\nprint (x1)\nprint (x2)\nprint()\nprint()\n# dim=0으로 연결\nx_concat = torch.cat([x1, x2], dim=0)\nprint(\"\\nConcatenated Tenspr along dim 0 (6x3):\")\nprint(x_concat)\nprint(x_concat.size())\nprint()\nprint()\n# dim=1로 연결\nx_concat_dim1 = torch.cat([x1, x2], dim=1)\nprint(\"\\nConcatenated Tensor along dim 1 (3x6):\")\nprint(x_concat_dim1)\nprint(x_concat_dim1.size())\n\n#stack으로 차원을 추가하고 쌓기\nx_stack = torch.stack([x1, x2], dim=0)\nprint(x_stack)\nprint(\" InStacked Tensor (2x3x3):\")\nprint(x_stack.size())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:56:18.801044Z","iopub.execute_input":"2024-10-31T13:56:18.801436Z","iopub.status.idle":"2024-10-31T13:56:18.816451Z","shell.execute_reply.started":"2024-10-31T13:56:18.801400Z","shell.execute_reply":"2024-10-31T13:56:18.815526Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\nTwo 3x3 Tensors:\ntensor([[-0.2057, -0.1644,  1.8213],\n        [-1.1225, -0.2836,  0.6554],\n        [-1.1674, -0.5278,  0.7946]])\ntensor([[-1.9592, -1.3165,  1.0435],\n        [-0.6125,  1.1163,  1.5921],\n        [-0.2526,  0.3875, -0.5105]])\n\n\n\nConcatenated Tenspr along dim 0 (6x3):\ntensor([[-0.2057, -0.1644,  1.8213],\n        [-1.1225, -0.2836,  0.6554],\n        [-1.1674, -0.5278,  0.7946],\n        [-1.9592, -1.3165,  1.0435],\n        [-0.6125,  1.1163,  1.5921],\n        [-0.2526,  0.3875, -0.5105]])\ntorch.Size([6, 3])\n\n\n\nConcatenated Tensor along dim 1 (3x6):\ntensor([[-0.2057, -0.1644,  1.8213, -1.9592, -1.3165,  1.0435],\n        [-1.1225, -0.2836,  0.6554, -0.6125,  1.1163,  1.5921],\n        [-1.1674, -0.5278,  0.7946, -0.2526,  0.3875, -0.5105]])\ntorch.Size([3, 6])\ntensor([[[-0.2057, -0.1644,  1.8213],\n         [-1.1225, -0.2836,  0.6554],\n         [-1.1674, -0.5278,  0.7946]],\n\n        [[-1.9592, -1.3165,  1.0435],\n         [-0.6125,  1.1163,  1.5921],\n         [-0.2526,  0.3875, -0.5105]]])\n InStacked Tensor (2x3x3):\ntorch.Size([2, 3, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n# 2x3x4 텐서 생성\nx = torch. rand(2, 3, 4)\nprint (x)\nprint(\"\\nOriginal Tensor (2x3x4):\")\nprint(x.size())\nprint()\nprint()\n# 전체 텐서의 합\nx_sum = x.sum()\nprint(\"\\nSum of all elements:\")\nprint(x_sum)\nprint()\n\n\n#dim=0 기준으로 합 계산 (2번째 축제거, 남은 것은 3x4 텐서)\nx_sum_dim0 = x.sum(dim=0)\nprint(\"\\nSum along dim 0 (3x4):\")\nprint(x_sum_dim0)\nprint(x_sum_dim0.size())\nprint()\n\n#dim=1 기준으로 합 계산 (3번째 축 제거, 남은 것은 2x4 텐서)\nx_sum_dim1 = x.sum(dim=1)\nprint(\"InSum along dim 1 (2x4):\")\nprint(x_sum_dim1)\nprint(x_sum_dim1.size())\nprint()\n\n#dim=2 기준으로 합 계산 (마지막 축 제거, 남은 것은 2x3 텐서)\nx_sum_dim2 = x.sum (dim=2)\nprint(\"\\nSum along dim 2 (2x3):\")\nprint (x_sum_dim2)\nprint(x_sum_dim2.size())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T13:59:18.511121Z","iopub.execute_input":"2024-10-31T13:59:18.511544Z","iopub.status.idle":"2024-10-31T13:59:18.524704Z","shell.execute_reply.started":"2024-10-31T13:59:18.511497Z","shell.execute_reply":"2024-10-31T13:59:18.523632Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"tensor([[[0.2031, 0.9021, 0.6017, 0.9027],\n         [0.0446, 0.5455, 0.3426, 0.7457],\n         [0.7902, 0.7374, 0.6732, 0.7650]],\n\n        [[0.7409, 0.1869, 0.9165, 0.4131],\n         [0.7795, 0.3390, 0.0834, 0.3339],\n         [0.5181, 0.1003, 0.7537, 0.0440]]])\n\nOriginal Tensor (2x3x4):\ntorch.Size([2, 3, 4])\n\n\n\nSum of all elements:\ntensor(12.4631)\n\n\nSum along dim 0 (3x4):\ntensor([[0.9440, 1.0891, 1.5182, 1.3158],\n        [0.8240, 0.8845, 0.4260, 1.0796],\n        [1.3083, 0.8376, 1.4269, 0.8090]])\ntorch.Size([3, 4])\n\nInSum along dim 1 (2x4):\ntensor([[1.0379, 2.1850, 1.6175, 2.4135],\n        [2.0385, 0.6262, 1.7536, 0.7910]])\ntorch.Size([2, 4])\n\n\nSum along dim 2 (2x3):\ntensor([[2.6097, 1.6784, 2.9658],\n        [2.2575, 1.5358, 1.4161]])\ntorch.Size([2, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"# 전체 텐서의 평균\nx_mean = x.mean()\nprint(\"\\nMean of all elements:\")\nprint(x_mean)\nprint()\n#dim=0 기준으로 평균 계산 (3x4 텐서)\nx_mean_dim0 = x.mean(dim=0)\nprint(\"\\nMean along dim 0 (3x4):\")\nprint(x_mean_dim0)\nprint(x_mean_dim0.size())\nprint()\nprint()\n# dim=1 기준으로 평균 계산 (2x4 텐서)\nx_mean_dim1 = x.mean(dim=1)\nprint(\"\\nMean along dim 1 (2x4):\")\nprint(x_mean_dim1)\nprint(x_mean_dim1.size())\n\n#dim=2 기준으로 평균 계산 (2x3 텐서)\nx_mean_dim2 = x.mean (dim=2)\nprint(\"\\nMean along dim 2 (2x3):\")\nprint (x_mean_dim2)\nprint(x_mean_dim2.size())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T14:00:45.403488Z","iopub.execute_input":"2024-10-31T14:00:45.404140Z","iopub.status.idle":"2024-10-31T14:00:45.414997Z","shell.execute_reply.started":"2024-10-31T14:00:45.404099Z","shell.execute_reply":"2024-10-31T14:00:45.413936Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"\nMean of all elements:\ntensor(0.5193)\n\n\nMean along dim 0 (3x4):\ntensor([[0.4720, 0.5445, 0.7591, 0.6579],\n        [0.4120, 0.4423, 0.2130, 0.5398],\n        [0.6542, 0.4188, 0.7134, 0.4045]])\ntorch.Size([3, 4])\n\n\n\nMean along dim 1 (2x4):\ntensor([[0.3460, 0.7283, 0.5392, 0.8045],\n        [0.6795, 0.2087, 0.5845, 0.2637]])\ntorch.Size([2, 4])\n\nMean along dim 2 (2x3):\ntensor([[0.6524, 0.4196, 0.7414],\n        [0.5644, 0.3839, 0.3540]])\ntorch.Size([2, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}