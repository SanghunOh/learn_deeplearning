{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7-1_implement_Word2Vec_keras.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov4s--WubGAg"
      },
      "source": [
        "### from https://youtu.be/L4p-ju44spQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk3q9W0DjH5G"
      },
      "source": [
        "# 케라스 Word2Vec 구현\n",
        "\n",
        "* 참고: https://wikidocs.net/69141"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxZOa3m8qFw3"
      },
      "source": [
        "### 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohSE_JnMSRDR"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                            remove=('headers', 'footers', 'quites'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E56d3KXroenQ"
      },
      "source": [
        "documents = dataset.data\n",
        "documents[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCo9S9RlrfQg"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xzZu6Iprj5B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi3PJ7dMsF8K"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "news_df = pd.DataFrame({'article':documents})\n",
        "len(news_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yv59GZAfJL9"
      },
      "source": [
        "# news_df\n",
        "news_df[news_df['article'].apply(len) < 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMCYgUIpSREH"
      },
      "source": [
        "import re\n",
        "def clean_text(doc):\n",
        "  pattern = '[^a-zA-Z\\s]'\n",
        "  text = re.sub(pattern, '', doc)\n",
        "  return text\n",
        "\n",
        "news_df['clean_text'] = news_df['article'].apply(clean_text)\n",
        "news_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI45-8AoZxR_"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def clean_stopword(doc):\n",
        "  stop_words = stopwords.words('english')\n",
        "  return ' '.join([w.lower() for w in doc.split() if w not in stop_words and len(w) > 3])\n",
        "\n",
        "news_df['clean_stopword'] = news_df['clean_text'].apply(clean_stopword)\n",
        "news_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSt1Tq7TZzmn"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize(doc):\n",
        "  return word_tokenize(doc)\n",
        "\n",
        "tokenzied_news = news_df['clean_stopword'].apply(tokenize)\n",
        "tokenzied_news = tokenzied_news.to_list()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "drop_news = [index for index, sentence in enumerate(tokenzied_news) if len(sentence) <= 1]\n",
        "news_texts = np.delete(tokenzied_news, drop_news, axis=0)\n",
        "len(news_texts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x1kmaB-ZzRI"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "news_20000 = news_texts[:20000]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(news_20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KjTvTNBkphC"
      },
      "source": [
        "idx2word = {value:key for key, value in tokenizer.word_index.items()}\n",
        "sequences = tokenizer.texts_to_sequences(news_20000)\n",
        "len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVsJkHxLlJZJ"
      },
      "source": [
        "print(sequences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydQWu3ZnqUc4"
      },
      "source": [
        "### Skipgram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOCiR_WrqMXk"
      },
      "source": [
        "#### Skipgram 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua1h2LF_8DPH"
      },
      "source": [
        "* 네거티브 샘플링(Negative Sampling)\n",
        "\n",
        "  + Word2Vec은 출력층이 내놓는 값에 소프트맥스 함수를 적용해 확률값으로 변환한 후 이를 정답과 비교해 역전파(backpropagation)\n",
        "  + 소프트맥스를 적용하려면 분모에 해당하는 값, 즉 중심단어와 나머지 모든 단어의 내적을 한 뒤, 이를 다시 exp 계산을 하는데 전체 단어가 많을 경우 엄청난 계산량 발생\n",
        "  + 네거티브 샘플링은 소프트맥스 확률을 구할 때 전체 단어를 대상으로 구하지 않고, 일부 단어만 뽑아서 계산을 하는 방식\n",
        "  + 네거티브 샘플링 동작은 사용자가 지정한 윈도우 사이즈 내에 등장하지 않는 단어(negative sample)를 5~20개 정도 뽑고, 이를 정답단어와 합쳐 전체 단어처럼 소프트맥스 확률을 계산하여 파라미터 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVkAK1ZlXi9Q"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "skip_grams_sample = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in sequences[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS3dg-yUoFyD"
      },
      "source": [
        "paris, labels = skip_grams_sample[0][0], skip_grams_sample[0][1]\n",
        "print(paris)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NBBbnyjXjUv"
      },
      "source": [
        "for i in range(5):\n",
        "  print('{}({}), {}({}) -> {}'.format(\n",
        "      idx2word[paris[i][0]], paris[i][0],\n",
        "      idx2word[paris[i][1]], paris[i][1],\n",
        "      labels[i]\n",
        "  ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsIyKuKuqQpp"
      },
      "source": [
        "#### Skipgram 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y222fSJx2CwH"
      },
      "source": [
        "skip_grams = [skipgrams(seq, vocabulary_size=vocab_size, window_size=10) for seq in sequences[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQAT9ZdwPktd"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dot\n",
        "from tensorflow.keras.utils import  plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A4aGPBYPl-v"
      },
      "source": [
        "embed_size = 50\n",
        "def word2vec():\n",
        "  target_inputs = Input(shape=(1,), dtype='int32')\n",
        "  target_embedding = Embedding(vocab_size, embed_size)(target_inputs)\n",
        "\n",
        "  context_inputs = Input(shape=(1,), dtype='int32')\n",
        "  context_embedding = Embedding(vocab_size, embed_size)(context_inputs)\n",
        "\n",
        "  dot_product = Dot(axes=2)([target_embedding, context_embedding])\n",
        "  dot_product = Reshape((1,), input_shape=(1,1))(dot_product)\n",
        "\n",
        "  output = Activation('sigmoid')(dot_product)\n",
        "\n",
        "  model = Model(inputs=[target_inputs, context_inputs], outputs=output)\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyyg1P7oPoM9"
      },
      "source": [
        "model = word2vec()\n",
        "model.summary()\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKaWUC2DPpMG"
      },
      "source": [
        "model.fit "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPNLRZqwXSib"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmzFckXv9nk2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omo9v74q9oKD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLfmfO1HqZxn"
      },
      "source": [
        "### CBOW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIrUD6Ph74ya"
      },
      "source": [
        "#### CBOW 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNBmb-UK9tJ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEIHgfWeALbL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPnygNQO1vjJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7dyMzMz1xXd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ulkMuv79SL"
      },
      "source": [
        "#### CBOW 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMC5u4Pm8Glt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yVXxpfs8JJX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POXBaE0d8LFe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK4u6zG49gVn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6fQuZSs9hqM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}