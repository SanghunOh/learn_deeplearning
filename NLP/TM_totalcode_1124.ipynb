{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TM_totalcode_1124.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE2pC44szhPv"
      },
      "source": [
        "## from book - 파이썬으로 텍스트 분석하기(늘봄)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1jaHym8zaBL"
      },
      "source": [
        "# 네이버 블로그 내용 가져오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQnkew3SzaBO"
      },
      "source": [
        "## 스크래핑을 위한 라이브러리\n",
        "네이버 검색 API와 Beautiful Soup 4를 이용하여 네이버 블로그 텍스트 데이터를 수집하기 위해서는 먼저 스크래핑에 필요한 여러 라이브러리를 import해야한다. 정규표현식을 사용하기 위해 re 라이브러리를 import하고, 네이버 API를 통해 가져온 JSON 파일을 처리하기위해 json 라이브러리를 import한다. 그리고 수학 라이브러리 math와 http 통신을 위해서 사용하는 requests 라이브러리를 import하고, 웹과 관련된 내용을 편리하게 처리하기 위해 urllib 라이브러리를 import 한다. 마지막으로 HTML과 XML을 파싱하는 데에 사용되는 파이썬 라이브러리 BeautifulSoup를 import하여 블로그 내용을 가져오기 위해 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xiZUFpQzaBP"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import math\n",
        "import requests\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "import urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE1klf8izaBQ"
      },
      "source": [
        "## 네이버 개발자 사이트에서 ID와 Secret 얻기\n",
        "네이버에서 제공하는 API를 이용하려면 네이버 개발자 사이트 https://developers.naver.com 에서 어플리케이션 등록 후, Client ID는 naver_clinet_id에 넣고, Client Secret은 naver_client_secret에 넣는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LI00w2PzaBR"
      },
      "source": [
        "naver_client_id = \"4zjZ2m1cpG8bsWtiVx0X\"\n",
        "naver_client_secret = \"yPdLCpxrca\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v9ZzSyLzaBR"
      },
      "source": [
        "## 네이버 블로그 갯수 가져오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z86VJHtxzaBS"
      },
      "source": [
        "def get_blog_count(query, display): #검색어 해당하는 블로그 전체 개수 가져오기\n",
        "    encode_query = urllib.parse.quote(query)\n",
        "    search_url = \"https://openapi.naver.com/v1/search/blog?query=\" + encode_query\n",
        "    request = urllib.request.Request(search_url) \n",
        "\n",
        "    request.add_header(\"X-Naver-Client-Id\", naver_client_id)  #헤더추가\n",
        "    request.add_header(\"X-Naver-Client-Secret\", naver_client_secret)\n",
        "\n",
        "    response = urllib.request.urlopen(request) #실제 사이트에 해당하는 결과 요청\n",
        "    response_code = response.getcode()  #반환되는 코드 확인\n",
        "\n",
        "    if response_code is 200: # 서버가 요청을 제대로 처리한 경우\n",
        "        response_body = response.read()\n",
        "        response_body_dict = json.loads(response_body.decode('utf-8'))  # 네이버가 제공하는 JSON파일로 읽기\n",
        "\n",
        "        print(\"Last build date: \" + str(response_body_dict['lastBuildDate']))  # JSON 출력 중 해당 항목의 필드를 가져와 출력\n",
        "        print(\"Total: \" + str(response_body_dict['total']))\n",
        "        print(\"Start: \" + str(response_body_dict['start']))\n",
        "        print(\"Display: \" + str(response_body_dict['display']))\n",
        "\n",
        "        if response_body_dict['total'] == 0:\n",
        "            blog_count = 0\n",
        "        else:\n",
        "            blog_total = math.ceil(response_body_dict['total'] / int(display))\n",
        "\n",
        "            if blog_total >= 1000:   # 검색결과를 최대 1000개까지 출력\n",
        "                blog_count = 1000\n",
        "            else:\n",
        "                blog_count = blog_total\n",
        "\n",
        "            print(\"Blog total: \" + str(blog_total))\n",
        "            print(\"Blog count: \" + str(blog_count))\n",
        "\n",
        "        return blog_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI-cwe-KzaBT"
      },
      "source": [
        "## 네이버 블로그 포스트 가져오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QixQkJNzaBU"
      },
      "source": [
        "def get_blog_post(query, display, start_index, sort):  # 검색된 블로그 내용 가져오기\n",
        "    global no, df\n",
        "    \n",
        "    encode_query = urllib.parse.quote(query)  # 검색API에 요청할 변수로 query외에 display, start, sort 추가\n",
        "    search_url = \"https://openapi.naver.com/v1/search/blog?query=\" + encode_query + \"&display=\" + str(display) + \"&start=\" + str(start_index) + \"&sort=\" + sort\n",
        "\n",
        "    request = urllib.request.Request(search_url)\n",
        "\n",
        "    request.add_header(\"X-Naver-Client-Id\", naver_client_id)  # 아이디와 비밀번호를 헤더에 추가\n",
        "    request.add_header(\"X-Naver-Client-Secret\", naver_client_secret)\n",
        "\n",
        "    response = urllib.request.urlopen(request)\n",
        "    response_code = response.getcode()\n",
        "\n",
        "    if response_code is 200:\n",
        "        response_body = response.read()\n",
        "        response_body_dict = json.loads(response_body.decode('utf-8'))\n",
        "        for item_index in range(0, len(response_body_dict['items'])):\n",
        "            try:                              # item 필드 안에 있는 title 등의 필드에 대한 내용 가져오기\n",
        "                remove_html_tag = re.compile('<.*?>') #HTML 태그 제거를 위한 정규표현식 함수 사용\n",
        "                title = re.sub(remove_html_tag, '', response_body_dict['items'][item_index]['title'])\n",
        "                link = response_body_dict['items'][item_index]['link'].replace(\"amp;\", \"\")\n",
        "                description = re.sub(remove_html_tag, '', response_body_dict['items'][item_index]['description'])\n",
        "                blogger_name = response_body_dict['items'][item_index]['bloggername']\n",
        "                blogger_link = response_body_dict['items'][item_index]['bloggerlink']\n",
        "                post_date = response_body_dict['items'][item_index]['postdate']\n",
        "\n",
        "                no += 1\n",
        "                post_code = requests.get(link)\n",
        "                post_text = post_code.text\n",
        "                post_soup = BeautifulSoup(post_text, 'lxml')\n",
        "\n",
        "                blog_post_content_text = \"\"\n",
        "                for mainFrame in post_soup.select('iframe#mainFrame'):\n",
        "                    blog_post_url = \"http://blog.naver.com\" + mainFrame.get('src')\n",
        "                    blog_post_code = requests.get(blog_post_url)\n",
        "                    blog_post_text = blog_post_code.text\n",
        "                    blog_post_soup = BeautifulSoup(blog_post_text, 'lxml')\n",
        "                    \n",
        "                    for blog_post_content in blog_post_soup.find_all('div', class_='se-viewer'):\n",
        "                        blog_post_content_text = blog_post_content.get_text()\n",
        "                        \n",
        "                    for blog_post_content in blog_post_soup.find_all('div', class_='se_doc_viewer'):\n",
        "                        blog_post_content_text = blog_post_content.get_text()\n",
        " \n",
        "                    for blog_post_content in blog_post_soup.select('div#postViewArea'):\n",
        "                        blog_post_content_text = blog_post_content.get_text()\n",
        "\n",
        "                df.loc[no] = [title, link, description, blogger_name, blogger_link, post_date, blog_post_content_text]\n",
        "                print(\"#\", end='')\n",
        "                \n",
        "            except:\n",
        "                item_index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7icXE-TzaBV"
      },
      "source": [
        "## 네이버 블로그 스크래핑 시작하고 data 하위폴더에 저장하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8lgc8hAzaBW",
        "outputId": "7f245fce-7a32-45b7-950d-1db820e5e911"
      },
      "source": [
        "no = 0                 # 몇개의 포스트를 저장하였는지 세기 위한 index\n",
        "query = \"스마트폰\"   # 검색을 원하는 문자열로서 UTF-8로 인코딩한다.\n",
        "display = 10           # 검색 결과 출력 건수 지정, 10(기본값),100(최대)\n",
        "start = 1              # 검색 시작 위치로 최대 1000까지 가능\n",
        "sort = \"sim\"           # 정렬 옵션: sim(유사도순, 기본값), date(날짜순)\n",
        "\n",
        "# 블로그를 DataFrame에 저장\n",
        "df = pd.DataFrame(columns=(\"Title\", \"Link\", \"Description\", \"Blogger Name\", \"Blogger Link\", \"Post Date\", \"Post Contents\"))\n",
        "\n",
        "blog_count = get_blog_count(query, display)\n",
        "for start_index in range(start, blog_count + 1, display):\n",
        "    get_blog_post(query, display, start_index, sort)\n",
        "\n",
        "df.to_csv(\"./data/smtph_total.csv\", header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last build date: Tue, 17 Aug 2021 10:46:26 +0900\n",
            "Total: 6702043\n",
            "Start: 1\n",
            "Display: 10\n",
            "Blog total: 670205\n",
            "Blog count: 1000\n",
            "##########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHSlx9MAzaBX"
      },
      "source": [
        "## 저장한 텍스트 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQDDsa-yzaBX"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./data/smtph_total.csv', encoding='utf-8', engine='python')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCzaGabHzaBY"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ4ErPe9zaBY"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXygVEy2zaBY"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBo2bpNvzaBZ"
      },
      "source": [
        "# 자연어 처리 (영어)\n",
        "- 영어로 구성된 텍스트 데이터를 읽어오고 자연어 처리 과정으로 tokenization과 lemmatizing을 수행한다.\n",
        "- 한국어 데이터인 경우에는 이 부분을 생략하고 한국어 자연어 처리 부분으로 바로 건너간다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YThrPklvzaBZ"
      },
      "source": [
        "## 텍스트 데이터 가져오기\n",
        "필요한 데이터 처리를 위해 라이브러리로 pandas를 import 한다.\n",
        "data 폴더에 있는 영어 텍스트 파일인 CEO3.csv 파일을 가져온다. CEO3.csv 파일은 대표 기업들의 인사말을 모아놓은 데이터이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSg38eDbzaBZ"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./data/CEO3.csv', encoding='latin')\n",
        "df[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTtKfje4zaBZ"
      },
      "source": [
        "CEO3.csv 파일은 number, firm, publishing, country, text 속성을 가지고 있다. 여기서 text 속성에 대해서만 가져온다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxCIpeS1zaBZ"
      },
      "source": [
        "texts = df.get('text')\n",
        "texts[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslwagUOzaBa"
      },
      "source": [
        "texts.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPIzeJ6vzaBa"
      },
      "source": [
        "## Tokenize and Lemmatize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_54azJ1HzaBa"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhuSzAQizaBa"
      },
      "source": [
        "### 불용어(Stopwords) 가져오기\n",
        "불용어(stopword)를 제거하기 위해서는 nltk 라이브러리에서 제공하는 stopwords를 다운로드한다. 다운로드한 불용어 중에서 영어에 대한 부분을 가져온다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8AQCqNJzaBa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words.extend([])\n",
        "stop_words[0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWPVDv0CzaBa"
      },
      "source": [
        "### 불용어를 제외하여 tokenize와 lemmatize 수행\n",
        "nltk 라이브러리에 포함된 wordnet 라이브러리를 이용하여 각 영문 텍스트에 대해서 tokenizer와 lemmatizer를 수행한다. 그리고 토큰 중에서 불용어에 포함되지 않은 텍스트에 대해서만 추출하여 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJd3IUw3zaBb"
      },
      "source": [
        "from nltk import wordnet\n",
        "nltk.download('wordnet')\n",
        "preprocessed_texts = []\n",
        "for text in texts.values:\n",
        "    tokenized_text = tokenizer.tokenize(text.lower())\n",
        "    lemmatized_text = [lemmatizer.lemmatize(token) for token in tokenized_text]\n",
        "    stopped_text = [token for token in lemmatized_text if token not in stop_words]\n",
        "    preprocessed_texts.append(stopped_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii2d_h3IzaBb"
      },
      "source": [
        "### 토큰 카운트(Token Count)\n",
        "문서에서 추출한 각 토큰에 대해서 갯수를 파악하기 위해서 Counter 라이브러리를 import한다. 그리고 전처리된 텍스트로부터 토큰의 갯수를 파악하여 상위 20개의 토큰에 대해서만 출력하여 결과를 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9SrhHr2zaBb"
      },
      "source": [
        "from collections import Counter\n",
        "tokens = []\n",
        "for text in preprocessed_texts:\n",
        "    tokens.extend(text)\n",
        "\n",
        "counted_tokens = Counter(tokens)\n",
        "top_20 = counted_tokens.most_common(20)\n",
        "top_20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQu1bV9ZzaBb"
      },
      "source": [
        "# 자연어 처리 (한국어)\n",
        "\n",
        "\n",
        "한국어로 구성된 텍스트 데이터를 읽어오고 자연어 처리 과정으로 형태소 분석을 수행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJyF6rHWzaBb"
      },
      "source": [
        "## 텍스트 데이터 가져오기\n",
        "\n",
        "- 필요한 데이터 처리를 위해 라이브러리로 pandas를 import 한다. \n",
        "- BS4로 클로링한 한글 텍스트 데이터 파일을 불러온다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKN_vBOAzaBb"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./data/smtph_total.csv', encoding='utf-8')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NJDC6JHzaBc"
      },
      "source": [
        "csv 파일에서 블로그 포스트의 제목인 'Title'과 포스트 설명인 'Description'에 해당하는 부분만 가져와 posts 리스트에 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujzeLSmWzaBc"
      },
      "source": [
        "galexy_posts = df.get('Title') + \" \" + df.get('Description')\n",
        "galexy_post_date = df.get('Post Date')\n",
        "galexy_posts.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz1VS92zzaBc"
      },
      "source": [
        "galexy_posts.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV3Ec3kgzaBc"
      },
      "source": [
        "galexy_post_date.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8UcaRLtzaBc"
      },
      "source": [
        "galexy_post_date.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpuhAOqszaBc"
      },
      "source": [
        "## 형태소 분석을 이용한 명사 추출\n",
        "\n",
        "한국어에 대한 자연어 처리를 위해서 대표적인 형태소 분석기 중 하나인 은전한잎(Mecab)을 사용한다. 은전한입 라이브러리를 이용하기 위해서는 pip install eunjeon 명령어를 실행해서 설치하도록 한다. 설치 후에는 Mecab 라이브러리를 import하고, Mecab() 클래스로 tagger를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_dI-VWSzaBc",
        "outputId": "3cf76500-371b-4634-96a1-e43c5ea84661"
      },
      "source": [
        "# !python3 -m pip install eunjeon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: eunjeon in /home/freesky/.local/lib/python3.6/site-packages (0.4.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.2 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFUnPduzzaBd"
      },
      "source": [
        "!python -m pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jI4yRDcSzaBd",
        "outputId": "e722bb99-a3d1-4c96-9a09-f3ed1361b490"
      },
      "source": [
        "# from eunjeon import Mecab\n",
        "from konlpy.tag import Mecab\n",
        "tagger = Mecab()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "The MeCab dictionary does not exist at \"/home/freesky/.local/lib/python3.6/site-packages/eunjeon/data/mecabrc\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/eunjeon/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--rcfile %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/eunjeon/mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/eunjeon/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Sometimes it works when we try twice.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--rcfile %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/eunjeon/mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-933aec681e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meunjeon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.6/site-packages/eunjeon/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--rcfile %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The MeCab dictionary does not exist at \"%s\". Is the dictionary correctly installed?\\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab(\\'/some/dic/path\\')\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Install MeCab in order to use it: https://github.com/koshort/pyeunjeon/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The MeCab dictionary does not exist at \"/home/freesky/.local/lib/python3.6/site-packages/eunjeon/data/mecabrc\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nynJLFdSzaBd"
      },
      "source": [
        "### 불용어(Stopwords) 사전 만들기\n",
        "일반적으로 형태소 분석을 통해 필요한 형태만 가져오면 자연스럽게 조사, 접속사 등이 제거되게 된다. 하지만 한국어를 분석하다 보면 명사에서도 상당히 많은 불필요한 단어들이 들어가는 것을 알 수 있다. 결국 사용자가 직접 불용어 사전을 유지하면서 불필요한 단어를 제거해야 하는 필요성이 생긴다. 직접 불용어를 등록하면서 형태소 분석에서 제외할 수 있도록 사전을 만들어본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1m1ubNFzaBd"
      },
      "source": [
        "# 각자 데이터에 맞는 불용어 리스트로 대체하여 불용어 처리.\n",
        "galexy_stop_words = \"강, 글, 애 미 번 은 이 것 등 더 를 좀 즉 인 옹 때 만 원 이때 개 일 기 시 럭 갤 성 삼 스 폰 트 드 기 이 리 폴 사 전 마 자 플 블 가 중 북 수 팩 년 월 저 탭\"\n",
        "galexy_stop_words = galexy_stop_words.split(' ')\n",
        "print(galexy_stop_words[0:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSL8JnT9zaBd"
      },
      "source": [
        "### 불용어를 제외하여 형태소 분석 수행\n",
        "eunjeon 라이브러리를 이용하여 한글 텍스트에 대해서 형태소 분석을 수행한다. 그리고 분석으로 추출하는 명사 중에서 불용어에 포함되지 않은 텍스트에 대해서만 추출하여 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NAjHwNyzaBd"
      },
      "source": [
        "words = []\n",
        "for post in galexy_posts:\n",
        "    words.extend(tagger.pos(post))\n",
        "words[0:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX2hYst_zaBd"
      },
      "source": [
        "galexy_nouns = []   # 명사 추출하기\n",
        "for post in galexy_posts:\n",
        "        if type(post) == str:\n",
        "            for noun in tagger.nouns(post):\n",
        "                if noun not in galexy_stop_words:\n",
        "                    galexy_nouns.append(noun)\n",
        "            \n",
        "galexy_nouns[0:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf8AOWGAzaBe"
      },
      "source": [
        "# 핵심어 빈도 분석(Keyword Analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbZrAA8EzaBe"
      },
      "source": [
        "## 명사 등 품사 빈도 계산\n",
        "\n",
        "텍스트 데이터에서 단어 빈도가 높은 상위 20개에 대해서 추출한다. 단어 빈도 계산을 위해서 Counter 라이브러리를 import하고, 추출된 상위 20개의 단어 빈도는 dict 함수를 통해 딕셔너리 형태로 저장한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhAocKPXzaBe"
      },
      "source": [
        "### 명사 빈도 계산\n",
        "갯수를 파악하기 위해서 Counter 라이브러리를 import한다. 그리고 전처리된 텍스트로부터 명사의 갯수를 파악하기 위해 Counter(nouns) 결과를 저장한 후, most_common(30)함수를 이용하여 상위 30개의 명사에 대해서만 출력하여 결과를 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnojf5yfzaBe"
      },
      "source": [
        "from collections import Counter  # 상위 30개 명사 추출\n",
        "num_top_nouns = 30\n",
        "galexy_nouns_counter = Counter(galexy_nouns)\n",
        "galexy_top_nouns = galexy_nouns_counter.most_common(num_top_nouns)\n",
        "galexy_top_nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrUkKrhYzaBe"
      },
      "source": [
        "galexy_top_nouns = dict(galexy_top_nouns) # 사전(dictionary) 형태로 변환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAEE8fxHzaBe"
      },
      "source": [
        "print(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac8EfoGmzaBe"
      },
      "source": [
        "# 유사어를 통페합한 후 명사 빈도수를 계산한다\n",
        "\n",
        "galexy_nouns_counter = Counter(galexy_nouns)\n",
        "galexy_nouns_counter['스마트폰'] +=galexy_nouns_counter['스마트']\n",
        "galexy_nouns_counter.pop('스마트')   # '스마트'를 '스마트폰'으로 간주하여 빈도수 계산하고 '스마트' 단어는 제외\n",
        "\n",
        "galexy_nouns_counter['삼성'] +=galexy_nouns_counter['삼성전자']\n",
        "galexy_nouns_counter.pop('삼성전자')   # '삼성전자'를 '삼성'으로 간주하여 빈도수 계산하고 '삼성전자' 단어는 제외\n",
        "\n",
        "num_top_nouns = 30\n",
        "galexy_top_nouns = galexy_nouns_counter.most_common(num_top_nouns)\n",
        "galexy_top_nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plCowfcmzaBe"
      },
      "source": [
        "galexy_top_nouns = dict(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuQUO7sVzaBf"
      },
      "source": [
        "### 동사 및 형용사 추출하고 빈도 계산하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgP1mpVszaBf"
      },
      "source": [
        "#### 형태소 분석을 한 결과에서 동사(VV)에 해당하는 형태에 '다'를 추가하여 출력한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH_hut7CzaBf"
      },
      "source": [
        "words = []\n",
        "for post in galexy_posts:\n",
        "    words.extend(tagger.pos(post))\n",
        "words[0:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSuLN-3zaBf"
      },
      "source": [
        "verbs=[]\n",
        "for word in words:\n",
        "    if word[1]=='VV':\n",
        "        verbs.append(word[0]+'다')\n",
        "print(verbs[0:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j13fBFByzaBf"
      },
      "source": [
        "형태소 분석을 한 결과에서 형용사(VA)에 해당하는 형태에 '다'를 추가하여 출력한 결과이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EJEpKgLzaBf"
      },
      "source": [
        "adjective=[]\n",
        "for word in words:\n",
        "    if word[1]=='VA':\n",
        "        adjective.append(word[0]+'다')\n",
        "print(adjective[0:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A2b5-yVzaBf"
      },
      "source": [
        " # 상위 30개 형용사 추출\n",
        "num_top_adj = 30\n",
        "galexy_adj_counter = Counter(adjective)\n",
        "galexy_top_adj = galexy_adj_counter.most_common(num_top_adj)\n",
        "galexy_top_adj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeUuuGBGzaBf"
      },
      "source": [
        "galexy_top_adj = dict(galexy_top_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckI1VLkuzaBf"
      },
      "source": [
        "## 단어 구름(Word Cloud)으로 표시하기\n",
        "\n",
        "단어구름으로 표시하기 위해서 matplotlib 라이브러리와 WordCloud 라이브러리를 import 한다. WordCloud 라이브러리는 pip install wordcloud 명령어를 통해서 설치해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z8SyWbgzaBg"
      },
      "source": [
        "!pip install wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9-ygE_MzaBg"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "galexy_wc = WordCloud(background_color=\"white\", font_path='./font/NanumBarunGothic.ttf')\n",
        "galexy_wc.generate_from_frequencies(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FeOsgPHzaBg"
      },
      "source": [
        "워드 클라우드를 생성하였으면 화면에 워드클라우드를 시각화하기 위해서 matplotlib를 이용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUysG1OczaBg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "figure = plt.figure()\n",
        "figure.set_size_inches(10, 10)\n",
        "ax = figure.add_subplot(1, 1, 1)\n",
        "ax.axis(\"off\")\n",
        "ax.imshow(galexy_wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLqoCyPSzaBg"
      },
      "source": [
        "### 형용사 빈도를 단어 구름으로 표시하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxdjnoQtzaBg"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "galexy_wc = WordCloud(background_color=\"white\", font_path='./font/NanumBarunGothic.ttf')\n",
        "galexy_wc.generate_from_frequencies(galexy_top_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LcLlAZlzaBg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "figure = plt.figure()\n",
        "figure.set_size_inches(10, 10)\n",
        "ax = figure.add_subplot(1, 1, 1)\n",
        "ax.axis(\"off\")\n",
        "ax.imshow(galexy_wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOK2D_I3zaBg"
      },
      "source": [
        "### 단어 구름 바탕 그림 변경하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmqo8QRjzaBg"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "cloud_mask=np.array(Image.open('./data/cloud.png')) \n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cloud_mask,interpolation=\"bilinear\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJlYFUwUzaBg"
      },
      "source": [
        "galexy_wc = WordCloud(background_color=\"white\", mask=cloud_mask, font_path='./font/NanumBarunGothic.ttf')\n",
        "galexy_wc.generate_from_frequencies(galexy_top_nouns)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(galexy_wc,interpolation=\"bilinear\")\n",
        "plt.axis(\"on\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goNe6TWPzaBh"
      },
      "source": [
        "## TF-IDF 계산\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdponNwizaBh"
      },
      "source": [
        "블로그 내용에서 빈도가 높은 단어에 대해서 TF-IDF 값을 구하기 위해서 TfidfVectorizer 라이브러리를 import 한다. 그리고 추출한 50개의 명사가 블로그 내용에서 어떤 TF-IDF 값을 가지는지 배열 형태로 출력한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4WOt2-6zaBh"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "galexy_tfidv = TfidfVectorizer().fit(galexy_top_nouns)\n",
        "galexy_tfidv.transform(galexy_posts).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfsHuf7lzaBh"
      },
      "source": [
        "TF-IDF 값에 대해서 테이블 형태로 살펴보기위해 DataFrame을 이용하여 출력한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLxfn_SyzaBh"
      },
      "source": [
        "pd.DataFrame(galexy_tfidv.transform(galexy_posts).toarray())\n",
        "df2=pd.DataFrame(galexy_tfidv.transform(galexy_posts).toarray())\n",
        "df2[100:110]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WUijHvazaBh"
      },
      "source": [
        "# 의미 연결망 분석(Semantic Network Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKSzDb-8zaBh"
      },
      "source": [
        "#### 주요 단어 20개를 가지고 의미 네트워크를 만들어 중심성 지수를 구하기 위해 먼저 필요한 라이브러리를 import 한다. 문장들을 나누기 위해 정규화 라이브러러 re를 import하고, 의미 네트워크를 사용하기 위해 networkx 라이브러리를 import 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njI-6BG5zaBh"
      },
      "source": [
        "import re\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjnXzZBszaBh"
      },
      "source": [
        "galexy_nouns = []\n",
        "for post in galexy_posts:\n",
        "      if type(post) == str:\n",
        "            for noun in tagger.nouns(post):\n",
        "                if noun not in galexy_stop_words:\n",
        "                    galexy_nouns.append(noun)\n",
        "            \n",
        "galexy_nouns[0:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT2HimgSzaBi"
      },
      "source": [
        "from collections import Counter  # 상위 20개 명사 추출\n",
        "num_top_nouns = 20\n",
        "galexy_nouns_counter = Counter(galexy_nouns)\n",
        "galexy_top_nouns = galexy_nouns_counter.most_common(num_top_nouns)\n",
        "galexy_top_nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebxThK7GzaBi"
      },
      "source": [
        "# 유사어를 통페합한 후 명사 빈도수를 계산한다\n",
        "\n",
        "galexy_nouns_counter = Counter(galexy_nouns)\n",
        "galexy_nouns_counter['스마트폰'] +=galexy_nouns_counter['스마트']\n",
        "galexy_nouns_counter.pop('스마트')   # '스마트'를 '스마트폰'으로 간주하여 빈도수 계산하고 '스마트' 단어는 제외\n",
        "\n",
        "galexy_nouns_counter['삼성'] +=galexy_nouns_counter['삼성전자']\n",
        "galexy_nouns_counter.pop('삼성전자')   # '삼성전자'를 '삼성'으로 간주하여 빈도수 계산하고 '삼성전자' 단어는 제외\n",
        "\n",
        "num_top_nouns = 20\n",
        "galexy_top_nouns = galexy_nouns_counter.most_common(num_top_nouns)\n",
        "galexy_top_nouns = dict(galexy_top_nouns)\n",
        "galexy_top_nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sDtQ3cvzaBi"
      },
      "source": [
        "print(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auFeIxkVzaBi"
      },
      "source": [
        "### 블로그 내용을 문장으로 나누기\n",
        "블로그 내용에 대해서 문장으로 나누기 위해서 문장의 끝을 나타내는 ';', '.', '?', '!'를 구분자로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5zAWI81zaBi"
      },
      "source": [
        "galexy_sentences = []\n",
        "for post in galexy_posts:\n",
        "    galexy_sentences.extend(re.split('; |\\.|\\?|\\!', post))\n",
        "galexy_sentences[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpzi1DUzaBi"
      },
      "source": [
        "### 문장별 명사 추출\n",
        "블로그 내용을 문장별로 구분하였고, 구분된 문장 별로 명사를 추출하여 정리한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLMqmciJzaBi"
      },
      "source": [
        "galexy_sentences_nouns = []\n",
        "for sentence in galexy_sentences:\n",
        "    sentence_nouns = tagger.nouns(sentence)\n",
        "    galexy_sentences_nouns.append(sentence_nouns)\n",
        "galexy_sentences_nouns[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYNksDiTzaBj"
      },
      "source": [
        "### 상위 단어에 대해 id 적용\n",
        "상위 단어 top_nouns에 대해서 key에 해당하는 단어, value에 해당하는 id를 넣어 딕셔너리 형태(word2id)로 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whgry5_xzaBj"
      },
      "source": [
        "galexy_word2id = {w: i for i, w in enumerate(galexy_top_nouns.keys())}\n",
        "galexy_word2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elmgGqMbzaBj"
      },
      "source": [
        "상위 단어 top_nouns에 대해서 key에 해당하는 id, value에 해당하는 단어를 넣어 딕셔너리 형태(id2word)로 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR_B0y7qzaBj"
      },
      "source": [
        "galexy_id2word = {i: w for i, w in enumerate(galexy_top_nouns.keys())}\n",
        "galexy_id2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFcAWFcqzaBj"
      },
      "source": [
        "### 인접행렬 생성\n",
        "상위 단어들에 대해서 상위 갯수만큼의 인접 행렬을 만들고, 문장 내에 상위 단어가 함께 포함된 비중에 따라 가중치를 계산하여 행렬에 표현한다. 인접 행렬을 생성하기 위해서 이전에 생성해둔 word2id를 이용한다. 행렬에서 만약 가중치가 0 이상이면 서로 연결되어 있음을 의미한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-UwU7J3zaBj"
      },
      "source": [
        "import numpy as np\n",
        "galexy_adjacent_matrix = np.zeros((num_top_nouns, num_top_nouns), int)\n",
        "for sentence in galexy_sentences_nouns:\n",
        "    for wi, i in galexy_word2id.items():\n",
        "        if wi in sentence:\n",
        "            for wj, j in galexy_word2id.items():\n",
        "                if i != j and wj in sentence:\n",
        "                    galexy_adjacent_matrix[i][j] += 1\n",
        "galexy_adjacent_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-57utESzaBj"
      },
      "source": [
        "인접 행렬에 대해서 네트워크를 만들고, 네트워크에 포함된 인접 행렬에 대한 결과를 살펴본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qorj-8AuzaBj"
      },
      "source": [
        "galexy_network = nx.from_numpy_matrix(galexy_adjacent_matrix)\n",
        "list(galexy_network.adjacency())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaNlL_pczaBj"
      },
      "source": [
        "생성된 네트워크에 대해서 갤럭시 데이터와 아이폰 데이터를 시각화하여 나타낸다. 여기서 labels 값으로 이전에 생성한 id2word를 이용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DajVCbLzaBk"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import font_manager as fm\n",
        "from matplotlib import rc\n",
        "\n",
        "font_name = 'Malgun Gothic'\n",
        "rc('font', family=font_name)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(20, 20)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.axis(\"off\")\n",
        "option = {\n",
        "    'node_color' : 'lightblue',\n",
        "    'node_size' : 2000,\n",
        "    'size' : 2\n",
        "}\n",
        "nx.draw(galexy_network, labels=galexy_id2word, font_family=font_name, ax=ax, **option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFwRDUQSzaBk"
      },
      "source": [
        "의미 네트워크를 표현하는 다양한 시각화 방법이 존재한다. 여기서는 Random Layout, Circular Layout, Spectral Layout, Spring Layout 형태로 시각화하여 표현한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4jD_ivUzaBk"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(20, 20)\n",
        "option = {\n",
        "    'node_color' : 'lightblue',\n",
        "    'node_size' : 500,\n",
        "    'size' : 100\n",
        "}\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.title('Random Layout', fontsize=20)\n",
        "nx.draw_random(galexy_network, labels=galexy_id2word, font_family=font_name, **option)\n",
        "plt.subplot(222)\n",
        "plt.title('Circular Layout', fontsize=20)\n",
        "nx.draw_circular(galexy_network, labels=galexy_id2word, font_family=font_name, **option)\n",
        "plt.subplot(223)\n",
        "plt.title('Spectral Layout',fontsize=20)\n",
        "nx.draw_spectral(galexy_network, labels=galexy_id2word, font_family=font_name, **option)\n",
        "plt.subplot(224)\n",
        "plt.title('Spring Layout',fontsize=20)\n",
        "nx.draw_spring(galexy_network, labels=galexy_id2word, font_family=font_name, **option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onC_zBGmzaBk"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(20, 20)\n",
        "option = {\n",
        "    'node_color' : 'lightblue',\n",
        "    'node_size' : 2000,\n",
        "    'size' : 100\n",
        "}\n",
        "\n",
        "plt.subplot(111)\n",
        "plt.title('Random Layout', fontsize=20)\n",
        "nx.draw_random(galexy_network, labels=galexy_id2word, font_family=font_name, **option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XObl_N_YzaBk"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(20, 20)\n",
        "option = {\n",
        "    'node_color' : 'lightblue',\n",
        "    'node_size' : 2000,\n",
        "    'size' : 100\n",
        "}\n",
        "\n",
        "plt.subplot(111)\n",
        "plt.title('Circular Layout', fontsize=20)\n",
        "nx.draw_circular(galexy_network, labels=galexy_id2word, font_family=font_name, **option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDd0uCqjzaBk"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(20, 20)\n",
        "option = {\n",
        "    'node_color' : 'lightblue',\n",
        "    'node_size' : 2000,\n",
        "    'size' : 100\n",
        "}\n",
        "\n",
        "plt.subplot(111)\n",
        "plt.title('Spectral Layout',fontsize=20)\n",
        "nx.draw_spectral(galexy_network, labels=galexy_id2word, font_family=font_name, **option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-DWYmtszaBk"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(20, 20)\n",
        "option = {\n",
        "    'node_color' : 'lightblue',\n",
        "    'node_size' : 2000,\n",
        "    'size' : 100\n",
        "}\n",
        "\n",
        "plt.subplot(111)\n",
        "plt.title('Spring Layout',fontsize=20)\n",
        "nx.draw_spring(galexy_network, labels=galexy_id2word, font_family=font_name, **option)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u52IRVsbzaBk"
      },
      "source": [
        "## 중심성 지수 구하기\n",
        "\n",
        "의미 네트워크에서 사용되는 여러 중심성 지수들이 있다. NetworkX 라이브러리에는 수 많은 중심성 지수 함수가 포함되어 있다. 이 중에서 degree centrality, eigenvector centrality, closeness centrality, current flow closeness centrality, current flow betweenness centrality, communicability betweenness centrality 중심성 지수를 수행해보도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi4FbIo7zaBl"
      },
      "source": [
        "#Degree 연결중심성\n",
        "nx.degree_centrality(galexy_network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6sF_0wmzaBl"
      },
      "source": [
        "def f2(x):\n",
        "   return x[1]\n",
        "\n",
        "res = sorted(nx.degree_centrality(galexy_network).items(), key=f2, reverse=True) # 내림차순 정렬\n",
        "for i in range(len(res)):\n",
        "    print(res[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwO_N7jZzaBl"
      },
      "source": [
        "#Current Flow Betweenness, 매개중심성\n",
        "nx.current_flow_betweenness_centrality(galexy_network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZqdA8OZzaBl"
      },
      "source": [
        "def f2(x):\n",
        "   return x[1]\n",
        "\n",
        "res = sorted(nx.current_flow_betweenness_centrality(galexy_network).items(), key=f2, reverse=True)\n",
        "#print(res)\n",
        "for i in range(len(res)):\n",
        "    print(res[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NNMbWDdzaBl"
      },
      "source": [
        "#Closeness, 근접중심성\n",
        "nx.closeness_centrality(galexy_network, distance='weight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ais5y4zaBl"
      },
      "source": [
        "res = sorted(nx.closeness_centrality(galexy_network, distance='weight').items(), key=f2, reverse=True)\n",
        "for i in range(len(res)):\n",
        "    print(res[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_IZ83hczaBl"
      },
      "source": [
        "#Eigenvector, 위세중심성\n",
        "nx.eigenvector_centrality(galexy_network, weight='weight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHIKEqjXzaBl"
      },
      "source": [
        "def f2(x):    # 함수정의 생략가능\n",
        "   return x[1]\n",
        "\n",
        "res = sorted(nx.eigenvector_centrality(galexy_network, weight='weight').items(), key=f2)  # 올림차순 정렬\n",
        "for i in range(len(res)):\n",
        "    print(res[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iph8TcZKzaBm"
      },
      "source": [
        "# 토픽 모델링(Topic Modeling)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grGRZ1fdzaBm"
      },
      "source": [
        "## Gensim 라이브러리\n",
        "토픽 모델링을 사용하기 위해서는 gensim 라이브러리를 사용해야 한다. gensim을 사용하기 위해서 pip install gensim 명령어를 통해 라이브러리를 설치한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZCu8jobzaBm"
      },
      "source": [
        "galexy_nouns = []   # 명사 추출하기\n",
        "nouns = []\n",
        "for post in galexy_posts:\n",
        "    if type(post) == str:\n",
        "        for noun in tagger.nouns(post):\n",
        "            if noun not in galexy_stop_words:\n",
        "                nouns.append(noun)\n",
        "        galexy_nouns.append(nouns)\n",
        "    else:\n",
        "        galexy_nouns.append(['nouns'])\n",
        "        \n",
        "galexy_nouns[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azQvUaAjzaBm"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo1zVp6CzaBm"
      },
      "source": [
        "####  명사에 대해서 bigram 형태로 만들어 사용한다. 그리고 bigram으로 묶인 명사에 대해서 trigram을 만들어 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mht5at90zaBm"
      },
      "source": [
        "import gensim\n",
        "galexy_bigram = gensim.models.Phrases(galexy_nouns)\n",
        "galexy_trigram = gensim.models.Phrases(galexy_bigram[galexy_nouns])\n",
        "galexy_bigram_model = gensim.models.phrases.Phraser(galexy_bigram)\n",
        "galexy_trigram_model = gensim.models.phrases.Phraser(galexy_trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e-HSmsgzaBm"
      },
      "source": [
        "from gensim import corpora\n",
        "galexy_bigram_document = [galexy_bigram_model[nouns] for nouns in galexy_nouns]\n",
        "galexy_bigram_document[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXsEjO7lzaBm"
      },
      "source": [
        "bigram 문서에 대해서 id와 word로 매핑된 딕셔너리를 생성한다. 그리고 doc2bow 함수를 통해서 (단어, 빈도수) 형태로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snkfzfklzaBm"
      },
      "source": [
        "galexy_id2word = corpora.Dictionary(galexy_bigram_document)\n",
        "galexy_corpus = [galexy_id2word.doc2bow(doc) for doc in galexy_bigram_document]\n",
        "galexy_corpus[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSAi_TfnzaBn"
      },
      "source": [
        "galexy_id2word[2000] #2000번째 단어가 무엇인지 확인"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNO3Pjg0zaBn"
      },
      "source": [
        "### Topic Coherence 계산\n",
        "토픽 모델링을 수행함에 있어서 적절한 토픽의 갯수를 찾는 것이 중요하다. 2부터 9까지 값을 늘려가면서 LDA 모델을 생성하여 각 모델의 coherence를 계산한다. 그리고 적절한 토픽의 수는 토픽 개수를 늘려가며 높은 coherence score를 가지는 값으로 결정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIMtoBlqzaBn"
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "galexy_coherence_score=[]\n",
        "for i in tqdm(range(2,10)):\n",
        "    model = gensim.models.ldamodel.LdaModel(corpus=galexy_corpus, id2word=galexy_id2word, num_topics=i)\n",
        "    coherence_model = CoherenceModel(model, texts=galexy_bigram_document, dictionary=galexy_id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model.get_coherence()\n",
        "    print('n=',i,'\\nCoherence Score: ', coherence_lda)\n",
        "    galexy_coherence_score.append(coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqJpsjuWzaBn"
      },
      "source": [
        "토픽의 적절한 개수를 찾기 위해서 계산한 coherence score를 차트로 나타내서 살펴본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ3o_SMRzaBn"
      },
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "import numpy\n",
        "\n",
        "k=[]\n",
        "for i in range(2,10):\n",
        "    k.append(i)\n",
        "\n",
        "x=numpy.array(k)\n",
        "y=numpy.array(galexy_coherence_score)\n",
        "plt.title('Galexy Topic Coherence')\n",
        "plt.plot(x,y)\n",
        "plt.xlim(2,10)\n",
        "plt.xlabel('Number Of Topic (2-10)')\n",
        "plt.ylabel('Cohrence Score')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoeBH5MfzaBn"
      },
      "source": [
        "### 토픽의 갯수가 증가할수록 계속적으로 coherence score가 증가하는 추이를 보인다. 증가 추이에서 가장 높은 값을 가지다가 낮아지기 시작하는 지점이 적절한 토픽의 갯수로 판단하고, LDA 모델을 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3jRF_GzzaBn"
      },
      "source": [
        "galexy_model = gensim.models.ldamodel.LdaModel(corpus=galexy_corpus, id2word=galexy_id2word, num_topics=4)\n",
        "galexy_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOJbV6a3zaBn"
      },
      "source": [
        "### pyLDAvis를 이용한 LDA 시각화 IDM\n",
        "LDA를 시각화하기 pyLDAvis 라이브러리를 import 해야 한다. pyLDAvis 라이브러리는 명령어 pip install pyLDAvis를 이용하여 설치해야 한다. 설치가 완료되면 실제 생성한 모델을 가지고 토픽간 거리지도(IDM)으로 시각화한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwr_TiP6zaBn"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDN-sr0kzaBo"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim as gensimvis\n",
        "prepared_data = gensimvis.prepare(galexy_model, galexy_corpus, galexy_id2word)\n",
        "pyLDAvis.display(prepared_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDYj4ERWzaBo"
      },
      "source": [
        "## 동적 토픽 모델링(Dynamic Topic Modeling)\n",
        "\n",
        "동적 토픽 모델링을 적용하기 위해서 텍스트 데이터에서 시간에 따른 분류로 전처리한다. 각 데이터는 특정 시간에 포함된 문서들의 단어들로 분류가 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K5QCEaCzaBo"
      },
      "source": [
        "galexy_nouns_date={}\n",
        "for i in range(len(galexy_posts)):\n",
        "    if galexy_post_date[i] in galexy_nouns_date:\n",
        "        galexy_nouns_date[galexy_post_date[i]].append(galexy_nouns[i])\n",
        "    else:\n",
        "        galexy_nouns_date[galexy_post_date[i]]=[]\n",
        "        galexy_nouns_date[galexy_post_date[i]].append(galexy_nouns[i])\n",
        "\n",
        "galexy_nouns_date = sorted(galexy_nouns_date.items())\n",
        "galexy_nouns_date[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBQWGrVpzaBo"
      },
      "source": [
        "시간에 따라 분류된 명사들이 있는 데이터에서 시간별로 명사들이 있는 문서의 개수를 계산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXby_Mi5zaBo"
      },
      "source": [
        "galexy_nouns_dates=[]\n",
        "galexy_date_length=[]\n",
        "for i in range(len(galexy_nouns_date)):\n",
        "    galexy_nouns_dates.append(galexy_nouns_date[i][1])\n",
        "    galexy_date_length.append(len(galexy_nouns_date[i][1]))\n",
        "\n",
        "galexy_date_length[0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZwLPrFwzaBo"
      },
      "source": [
        "시간별로 명사들이 있는 데이터를 하나로 통합한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdFVVc7yzaBo"
      },
      "source": [
        "galexy_merged_dates=[]\n",
        "for posts in galexy_nouns_dates:\n",
        "    for post in posts:\n",
        "        galexy_merged_dates=galexy_merged_dates+[post]\n",
        "\n",
        "galexy_merged_dates[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-vVHMZTzaBo"
      },
      "source": [
        "이전에 따로 수행하던 토픽 모델링에 필요한 과정을 docLDA 함수를 정의하여 사용한다. docLDA는 명사들을 bigram, trigram으로 만들고 LDA model, corpus, id2word를 생성한다. docLDA 함수를 사용하여 한번에 토픽 모델링에 필요한 요소들을 생성하고, 각 토픽들에 대해서 출력하여 살펴본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzCN2QrpzaBo"
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "def docLDA(nouns):\n",
        "    bigram = gensim.models.Phrases(nouns)\n",
        "    trigram = gensim.models.Phrases(bigram[nouns])\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "    bigram_document = [bigram_mod[doc] for doc in nouns]\n",
        "    id2word = corpora.Dictionary(bigram_document)\n",
        "\n",
        "    corpus = [id2word.doc2bow(doc) for doc in bigram_document]\n",
        "    model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=4)\n",
        "    \n",
        "    return model, corpus, id2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PSvlll53zaBo"
      },
      "source": [
        "galexy_model, galexy_corpus, galexy_id2word = docLDA(galexy_nouns)\n",
        "galexy_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlFK-0L9zaBp"
      },
      "source": [
        "getTopics 함수는 LDA 모델에 포함된 각 토픽들에 대해서 반환한다. 블로그 텍스트에서 getTopics 함수를 통해 토픽들을 살펴본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxsSJAtazaBp"
      },
      "source": [
        "def getTopics(model):\n",
        "    topics = []\n",
        "    for topic in model.print_topics():\n",
        "        i=1\n",
        "        model_words=[]\n",
        "        topic_words=str(topic).split('\"')\n",
        "        for words in topic_words:\n",
        "            if i%2==0:\n",
        "                model_words.append(words)\n",
        "            i+=1\n",
        "        topics.append(model_words)\n",
        "    return topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iTZT9T5zaBp"
      },
      "source": [
        "galexy_topics = getTopics(galexy_model)\n",
        "galexy_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86v1KkGxzaBp"
      },
      "source": [
        "동적 토픽 모델로 살펴보기 위해 문서마다 가지고 있는 토픽들의 크기를 알아야 한다. 여기서는 날짜별로 합쳐진 문서마다 토픽들의 크기를 계산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4jg9JrdzaBp"
      },
      "source": [
        "galexy_topic_size=[]\n",
        "index=0\n",
        "for post_len in galexy_date_length:\n",
        "    topic_size=[]\n",
        "    doc=[]\n",
        "    for i in range(post_len):\n",
        "        doc+=galexy_merged_dates[index]\n",
        "        index+=1\n",
        "    for model in galexy_topics:\n",
        "        count=0\n",
        "        for noun in doc:\n",
        "            if noun in model:\n",
        "                count+=1\n",
        "        topic_size.append(count)\n",
        "    galexy_topic_size.append(topic_size)\n",
        "\n",
        "galexy_topic_size[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEJ5JySnzaBp"
      },
      "source": [
        "Numpy의 배열 형태로 저장하기 위해 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPsLAYLQzaBp"
      },
      "source": [
        "import numpy as np\n",
        "galexy_topic_array=np.array(galexy_topic_size)\n",
        "galexy_topic_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF45DLCNzaBp"
      },
      "source": [
        "matplotlib를 이용하여 차트로 시각화하여 시간에 따른 토픽들의 변화를 살펴본다. 차트에서 x축은 시간이 나타내고, y축은 토픽의 크기가 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f8Bd3JVzaBp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "\n",
        "galexy_x=[]\n",
        "galexy_y=[]\n",
        "\n",
        "galexy_topic_1=galexy_topic_array[:,0]\n",
        "galexy_topic_2=galexy_topic_array[:,1]\n",
        "galexy_topic_3=galexy_topic_array[:,2]\n",
        "galexy_topic_4=galexy_topic_array[:,3]\n",
        "\n",
        "galexy_unique_date=sorted(list(set(galexy_post_date)))\n",
        "galexy_plt_data=[]\n",
        "for date in galexy_unique_date:\n",
        "    times= dt.datetime.strptime(str(date),'%Y%m%d')\n",
        "    galexy_x.append(times)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=(15,10)\n",
        "plt.title(\"Galexy DTM\")\n",
        "plt.plot(galexy_x, galexy_topic_1,'o-', label=\"Topic 1\")\n",
        "plt.plot(galexy_x, galexy_topic_2,'o-', label=\"Topic 2\")\n",
        "plt.plot(galexy_x, galexy_topic_3,'o-', label=\"Topic 3\")\n",
        "plt.plot(galexy_x, galexy_topic_4,'o-', label=\"Topic 4\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcHoFc-JzaBq"
      },
      "source": [
        "# Word2Vec\n",
        "Word2Vec은 단어 임베딩(embedding)을 하는 방법으로 여기서는 skip-gram 모델을 이용해 단어를 벡터로 변환하고 TSNE 알고리즘으로 시각화 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPm9arQtzaBq"
      },
      "source": [
        "### Word2Vec 생성 및 시각화\n",
        "Word2Vec을 생성하고 시각화하기 위해서는 gensim.models의 Word2Vec 라이브러리를 import한다. 그리고 시각화를 위해서 TSNE 라이브러리를 import한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4It9_ZrMzaBq"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import font_manager as fm\n",
        "from matplotlib import rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqWfIdzZzaBq"
      },
      "source": [
        "galexy_nouns = []   # 명사 추출하기\n",
        "nouns = []\n",
        "for post in galexy_posts:\n",
        "    if type(post) == str:\n",
        "        for noun in tagger.nouns(post):\n",
        "            if noun not in galexy_stop_words:\n",
        "                nouns.append(noun)\n",
        "        galexy_nouns.append(nouns)\n",
        "    else:\n",
        "        galexy_nouns.append(['nouns'])\n",
        "        \n",
        "galexy_nouns[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrnCnqnyzaBq"
      },
      "source": [
        "galexy_word2vec = Word2Vec(galexy_nouns, min_count=1)\n",
        "galexy_word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOZir92jzaBq"
      },
      "source": [
        "생성된 데이터의 Word2Vec 모델에서 핵심어와 유사한 단어를 출력한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpW6pz2DzaBq"
      },
      "source": [
        "galexy_word2vec.wv.most_similar(\"스마트폰\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNuXjuWUzaBq"
      },
      "source": [
        "Word2Vec을 2차원으로 시각화를 위해서 n_components가 2인 TSNE를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVDo5wabzaBq"
      },
      "source": [
        "galexy_tsne = TSNE(n_components=2)\n",
        "galexy_tsne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vBqzBaVzaBq"
      },
      "source": [
        "Word2Vec으로 나온 어휘에서 유사도를 계산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMlRp9gnzaBr"
      },
      "source": [
        "galexy_vocab = galexy_word2vec.wv.vocab\n",
        "galexy_similarity = galexy_word2vec[galexy_vocab]\n",
        "galexy_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY-lOlOczaBr"
      },
      "source": [
        "벡터의 유사도를 2차원으로 변환하여 DataFrame으로 살펴본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7C7kc4czaBr"
      },
      "source": [
        "galexy_transform_similarity = galexy_tsne.fit_transform(galexy_similarity)\n",
        "galexy_df = pd.DataFrame(galexy_transform_similarity, index=galexy_vocab, columns=['x', 'y'])\n",
        "galexy_df[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkjXkXvuzaBr"
      },
      "source": [
        "Word2Vec 모델에 대해서 TSNE를 통해 2차원으로 시각화하여 나타낸다. annotate(word, pos)로 벡터 위치에 단어를 annotation 하는 형태로 만들고, matplotlib를 이용하여 시각화한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm4ERU_CzaBr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import font_manager as fm\n",
        "from matplotlib import rc\n",
        "\n",
        "font_name = 'Malgun Gothic'\n",
        "rc('font', family=font_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPtM0fLuzaBr"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "fig.set_size_inches(20, 20)\n",
        "\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.scatter(galexy_df['x'], galexy_df['y'])\n",
        "for word, pos in galexy_df.iterrows():\n",
        "    ax.annotate(word, pos)\n",
        "ax.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFzGBavEzaBr"
      },
      "source": [
        "galexy_word2vec.wv.most_similar(\"스마트폰\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7m1W9vrzaBr"
      },
      "source": [
        "# 군집 분석\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcAoWY4pzaBr"
      },
      "source": [
        "### k-Means를 이용한 비계층적 군집분석\n",
        "비계층적 군집화를 위해서 먼저 KMeans 라이브러리와 AgglomerativeClustering 라이브러리를 import한다. 그리고 군집화를 하기 전에 matplotlib를 이용하여 단어를 벡터로 변환하여 시각화한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm44PieVzaBr"
      },
      "source": [
        "galexy_post_nouns = []   # 문서별로 명사 추출하기\n",
        "for post in galexy_posts:\n",
        "    if type(post) == str:\n",
        "        post_nouns = tagger.nouns(post)\n",
        "        for i in range(len(post_nouns)-1, 0, -1):\n",
        "            if post_nouns[i] in galexy_stop_words:\n",
        "                del post_nouns[i]\n",
        "        galexy_post_nouns.append(post_nouns)\n",
        "          \n",
        "galexy_post_nouns[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXF7FVQ5zaBr"
      },
      "source": [
        "# 문서별로 추출한 명사들을 띄어쓰기를 붙여서 목록형(list)으로 만들기\n",
        "for i in range(len(galexy_post_nouns)):\n",
        "    galexy_post_nouns[i] = ' '.join(galexy_post_nouns[i])\n",
        "\n",
        "print(galexy_post_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH0Xy6cyzaBr"
      },
      "source": [
        "# 군집분석에 필요한 라이브러리 설치\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import font_manager as fm\n",
        "from matplotlib import rc\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as shc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5-4XZUszaBs"
      },
      "source": [
        "#===============K means 클러스터 분석========================\n",
        "vec = CountVectorizer() # 단어빈도 기준으로 문서-단어 메트릭스DTM 생성\n",
        "X = vec.fit_transform(galexy_post_nouns)\n",
        "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmkN72HLzaBs"
      },
      "source": [
        "# 어휘빈도_문서역빈도(TF-IDF) 기준으로 문서-단어 메트릭스DTM 생성\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer() \n",
        "X = vec.fit_transform(galexy_post_nouns)\n",
        "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
        "df[10:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9CzOPPnzaBs"
      },
      "source": [
        "클러스터의 개수 n_clusters가 3개인 KMeans 모델을 생성한다. 그리고 모델을 통해 나온 결과를 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AufYUwzxzaBs"
      },
      "source": [
        "kmeans = KMeans(n_clusters=3).fit(df) # 군집수가 3인 비계층적 군집분석 시행\n",
        "kmeans.labels_ # 각 문서가 어느 군집에 속하는지 표시"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-CEIkCuzaBs"
      },
      "source": [
        "# 주성분분석(PCA)을 통해 데이터 분석결과를 2차원으로 변환하여 보여준다.\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(df)\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "\n",
        " \n",
        "principalDf.index=galexy_posts\n",
        "\n",
        "kmeans.labels_ == 0\n",
        "\n",
        "# x축 : first y출 : second 번호로 나타낸후 plot으로 시각화\n",
        "plt.scatter(principalDf.iloc[kmeans.labels_ == 0, 0], principalDf.iloc[kmeans.labels_ == 0, 1], s = 10, c = 'red', label = 'cluster1')\n",
        "plt.scatter(principalDf.iloc[kmeans.labels_ == 1, 0], principalDf.iloc[kmeans.labels_ == 1, 1], s = 10, c = 'blue', label = 'cluster2')\n",
        "plt.scatter(principalDf.iloc[kmeans.labels_ == 2, 0], principalDf.iloc[kmeans.labels_ == 2, 1], s = 10, c = 'green', label = 'cluster3')\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61cteEkyzaBs"
      },
      "source": [
        "## 계층적 군집분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKNolNOizaBs"
      },
      "source": [
        "### 와드에 의한 연결법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOf0CIQozaBs"
      },
      "source": [
        "# 와드연결법으로 군집이 3개가 되도록 만들어 각 문서가 속한 군집을 배열형 데이터로 반환\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=3, linkage='ward')  \n",
        "cluster.fit_predict(df)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQXBydALzaBs"
      },
      "source": [
        " # 와드연결법의 덴드로그램으로 시각화\n",
        "    \n",
        "plt.figure(figsize=(10, 7))  \n",
        "plt.title(\"Customer Dendograms\")  \n",
        "dend = shc.dendrogram(shc.linkage(df, method='ward'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cy-PYo0zaBs"
      },
      "source": [
        "### 완전 연결법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYQyzPDyzaBt"
      },
      "source": [
        "plt.figure(figsize=(30, 20))  # 완전연결법의 덴드로그램으로 시각화\n",
        "plt.title(\"Customer Dendograms\")  \n",
        "dend = shc.dendrogram(shc.linkage(df, method='complete'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg54WcFkzaBt"
      },
      "source": [
        "## 군집분석으로 나누어진 하위 집단에 대한 추가적인 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENHqJEzwzaBt"
      },
      "source": [
        "### 3개 하위군집으로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO2AgE0LzaBt"
      },
      "source": [
        "sub_gr = kmeans.labels_\n",
        "sub_gr[100:120]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M5RmGjMzaBt"
      },
      "source": [
        "sub_1, sub_2, sub_3 = [], [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnUZuDFazaBt"
      },
      "source": [
        "for i in range(len(sub_gr)):\n",
        "    if sub_gr[i] == 0:\n",
        "        sub_1.append(galexy_post_nouns[i])\n",
        "    elif sub_gr[i] == 1:\n",
        "        sub_2.append(galexy_post_nouns[i])\n",
        "    else:\n",
        "        sub_3.append(galexy_post_nouns[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAnKep6GzaBt"
      },
      "source": [
        "print(len(sub_1))\n",
        "print(sub_1[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMYrA2sdzaBt"
      },
      "source": [
        "print(len(sub_2))\n",
        "print(sub_2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-8tJTk9zaBt"
      },
      "source": [
        "print(len(sub_3))\n",
        "print(sub_3[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNW6aHj8zaBt"
      },
      "source": [
        "def devide(sub_x):\n",
        "    result = []\n",
        "    for i in range(len(sub_x)):\n",
        "        temp = sub_x[i].split(' ')\n",
        "        for j in range(len(temp)):\n",
        "            result.append(temp[j])\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1RisLTMzaBu"
      },
      "source": [
        "sub_1 = devide(sub_1)\n",
        "sub_2 = devide(sub_2)\n",
        "sub_3 = devide(sub_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCXTrIexzaBu"
      },
      "source": [
        "### 군집1의 핵심어 빈도분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik7CzBjVzaBu"
      },
      "source": [
        "from collections import Counter  # 상위 30개 명사 추출\n",
        "num_top_nouns = 30\n",
        "galexy_nouns_counter = Counter(sub_1)\n",
        "galexy_top_nouns = galexy_nouns_counter.most_common(num_top_nouns)\n",
        "galexy_top_nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69e8GptxzaBu"
      },
      "source": [
        "galexy_top_nouns = dict(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yA1_ggkzaBu"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "galexy_wc = WordCloud(background_color=\"white\", font_path='./font/NanumBarunGothic.ttf')\n",
        "galexy_wc.generate_from_frequencies(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gFw8gV5zaBu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "figure = plt.figure()\n",
        "figure.set_size_inches(10, 10)\n",
        "ax = figure.add_subplot(1, 1, 1)\n",
        "ax.axis(\"off\")\n",
        "ax.imshow(galexy_wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-tE5dbrzaBu"
      },
      "source": [
        "### 군집2의 핵심어 빈도분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jeu_Ww65zaBu"
      },
      "source": [
        "from collections import Counter  # 상위 30개 명사 추출\n",
        "num_top_nouns = 30\n",
        "galexy_nouns_counter = Counter(sub_2)\n",
        "galexy_top_nouns = galexy_nouns_counter.most_common(num_top_nouns)\n",
        "galexy_top_nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4r-0j1QzaBv"
      },
      "source": [
        "galexy_top_nouns = dict(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlr-cGeAzaBv"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "galexy_wc = WordCloud(background_color=\"white\", font_path='./font/NanumBarunGothic.ttf')\n",
        "galexy_wc.generate_from_frequencies(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29xlHgoozaBv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "figure = plt.figure()\n",
        "figure.set_size_inches(10, 10)\n",
        "ax = figure.add_subplot(1, 1, 1)\n",
        "ax.axis(\"off\")\n",
        "ax.imshow(galexy_wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YibdhJnvzaBv"
      },
      "source": [
        "### 군집3의 핵심어 빈도분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7oahbRlzaBv"
      },
      "source": [
        "from collections import Counter  # 상위 30개 명사 추출\n",
        "num_top_nouns = 30\n",
        "galexy_nouns_counter = Counter(sub_3)\n",
        "galexy_top_nouns = galexy_nouns_counter.most_common(num_top_nouns)\n",
        "galexy_top_nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWepRmnqzaBv"
      },
      "source": [
        "galexy_top_nouns = dict(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DADRnyjyzaBv"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "galexy_wc = WordCloud(background_color=\"white\", font_path='./font/NanumBarunGothic.ttf')\n",
        "galexy_wc.generate_from_frequencies(galexy_top_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvpuXGJezaBv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "figure = plt.figure()\n",
        "figure.set_size_inches(10, 10)\n",
        "ax = figure.add_subplot(1, 1, 1)\n",
        "ax.axis(\"off\")\n",
        "ax.imshow(galexy_wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5L5CZ-RzaBv"
      },
      "source": [
        "# 감성 분석(Sentiment Analysis)\n",
        "\n",
        "감성 분석을 위해서 긍정에 해당하는 딕셔너리 positive-words.txt와 부정에 해당하는 딕셔너리 negative-words.txt를 사용한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0MemUa7zaBw"
      },
      "source": [
        "dic1 = open('./dict/positive-words.txt')\n",
        "dic2 = open('./dict/negative-words.txt')\n",
        "positive_words=[]\n",
        "negative_words=[]\n",
        "for line in dic1:\n",
        "    positive_words.append(line.strip('\\n'))\n",
        "for line in dic2:\n",
        "    negative_words.append(line.strip('\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIUyjN62zaBw"
      },
      "source": [
        "positive_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKTRT9qbzaBw"
      },
      "source": [
        "negative_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLEmEDp8zaBw"
      },
      "source": [
        "영문으로 된 CEO들의 인사말에 대해서 감정분석을 수행하기 위해서 데이터를 읽는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUG_HCCXzaBw"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./data/CEO3.csv', encoding='latin')\n",
        "df[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Z73FkNzaBw"
      },
      "source": [
        "firm = df.get('firm')\n",
        "text = df.get('text')\n",
        "\n",
        "company = {firm[0]:text[0]}\n",
        "for i in range(len(firm)):\n",
        "    if company.get(firm[i]) != None:\n",
        "        company[firm[i]] = company.get(firm[i]) + \"\\n\" + text[i]\n",
        "    else:\n",
        "        company[firm[i]] = text[i]\n",
        "\n",
        "company.get('Amorepacific')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDV2_XlUzaBw"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ayoECBOzaBw"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokens=[]\n",
        "for f, t in company.items():\n",
        "    token=word_tokenize(t)\n",
        "    tokens.append([f,token])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFfYBsDKzaBx"
      },
      "source": [
        "감성 분석을 위해 각 토큰마다 긍정에 해당하면 +1하고 부정에 해당하면 -1로 계산하고, 전체 단어의 수로 나누어서 각 회사마다 긍정 부정의 정도를 계산한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPoW88lzaBx"
      },
      "source": [
        "sentiment_firm=[]\n",
        "\n",
        "for token in tokens:\n",
        "    firm=token[0]\n",
        "    sentiment=0\n",
        "    count=0\n",
        "    \n",
        "    for t in token[1]:\n",
        "        if t in positive_words:\n",
        "            sentiment+=1\n",
        "            count+=1\n",
        "        elif t in negative_words:\n",
        "            sentiment -=1\n",
        "            count+=1\n",
        "    \n",
        "    sentiment_firm.append([firm,sentiment/count])\n",
        "\n",
        "sentiment_firm[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jngw_r0JzaBx"
      },
      "source": [
        "각 회사마다 감성 분석된 결과를 %로 시각화하여 나타낸다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYXnQG4YzaBx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "a=[]\n",
        "for firm in sentiment_firm:\n",
        "    a.append(firm[1]*100)\n",
        "X=np.arange(len(a))\n",
        "\n",
        "plt.title(\"sentiment(%)\",fontsize=20)\n",
        "plt.bar(X,a)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1ROSxj1zaBx"
      },
      "source": [
        "## 감성분석(세분화)\n",
        "\n",
        "세분화된 감성 분석을 위해서는 py_lex 라이브러리를 설치해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlELwYikzaBx"
      },
      "source": [
        "!pip install py_lex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRjhZ3BHzaBx"
      },
      "source": [
        "py_lex에 있는 감정 어휘집을 사용하여 매우 다양한 세부 감성에 대해서 분석에 활용할 수 있다. 먼저 엑셀 파일로 감정 어휘집을 읽어온다. 읽어온 감성 어휘집을 보면 세분화된 감성으로 Positive, Negative, Anger, Anticipation, Disgust, Fear, Joy, Sadness, Surprise, Trust를 가지고 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upkX5czFzaBx"
      },
      "source": [
        "from py_lex import Liwc, EmoLex\n",
        "import pandas as pd\n",
        "emotion_dic=pd.read_excel('./dict/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations_ENG.xlsx')\n",
        "emotion_dic[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSRc4vYHzaBx"
      },
      "source": [
        "영문으로 된 CEO 인사말을 감성분석하기 위해서 영어로 된 감성 사전을 지정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vlU5fYozaBy"
      },
      "source": [
        "df=emotion_dic.set_index('English (en)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUKg7bP2zaBy"
      },
      "source": [
        "각 토큰에 대해서 감성 사전의 다양한 감성 정보로 세분화하여 계산한다. 즉, 각 회사마다 세분화된 감성의 정도를 계산한 결과를 가지게 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXK1DrGEzaBy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "emotion_firm=[]\n",
        "\n",
        "for token in tokens:\n",
        "    firm=token[0]\n",
        "    sentiment=0\n",
        "    count=0\n",
        "    arr=[]\n",
        "    senti_count=0\n",
        "    for t in token[1]:\n",
        "        if t in df.index:\n",
        "            count+=1\n",
        "            arr.append(list(df.loc[t]))\n",
        "            if np.sum(list(df.loc[t]))!=0:\n",
        "                senti_count+=1\n",
        "    emotion_firm.append([firm,np.sum(arr,axis=0),senti_count/count])\n",
        "\n",
        "emotion_firm[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KorbcTDzaBy"
      },
      "source": [
        "시각화하여 나타내기 위해 각 회사마다 계산된 결과를 세분화된 감성으로 분류하여 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xzGgquezaBy"
      },
      "source": [
        "emotion_result=[]\n",
        "for firm in emotion_firm:\n",
        "    emotion_result.append({\"Firm\" : firm[0],\n",
        "                           \"Positive\": str(\"%0.2f\"%( firm[1][0]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Negative\" : str(\"%0.2f\"%( firm[1][1]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Anger\" : str(\"%0.2f\"%( firm[1][2]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Anticipation\" : str(\"%0.2f\"%( firm[1][3]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Disgust\" : str(\"%0.2f\"%( firm[1][4]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Fear\" : str(\"%0.2f\"%( firm[1][5]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Joy\" : str(\"%0.2f\"%( firm[1][6]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Sadness\" : str(\"%0.2f\"%( firm[1][7]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Surprise\" : str(\"%0.2f\"%( firm[1][8]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Trust\" : str(\"%0.2f\"%( firm[1][9]/(firm[1][0]+firm[1][1])*100 ))+\"%\",\n",
        "                            \"Sentiwords / non-sentiwords (%)\":str(\"%0.2f\"%(firm[2]*100))+\"%\"}\n",
        "                          )\n",
        "emotion_result[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta41SDgtzaBy"
      },
      "source": [
        "세분화하여 감성 분석된 결과를 3차원으로 시각화하기 위해서 x, y, z 정보를 정의한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQc_BIg_zaBy"
      },
      "source": [
        "y=[]\n",
        "for firm in emotion_firm:\n",
        "    y.append(list(firm[1][2:]))\n",
        "x=['Positive','Negative','Anger','Anticipation','Disgust','Fear','Joy','Sadness','Suprise','Trust']\n",
        "z=[]\n",
        "for firm in emotion_firm:\n",
        "    z.append(firm[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22fZRd8_zaBy"
      },
      "source": [
        "세부 감성대로 나온 결과를 3차원으로 시각화한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XmRrf7kzaBy"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig=plt.figure()\n",
        "fig.set_size_inches(20, 20, 20)\n",
        "\n",
        "ax = fig.add_subplot(111,projection='3d')\n",
        "for firm in emotion_firm:\n",
        "    xs=np.arange(8)\n",
        "\n",
        "    ys=firm[1][2:]\n",
        "    zs=emotion_firm.index(firm)\n",
        "    ax.bar(xs,ys,zs,zdir='y',alpha=0.8)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaKqqKEuzaBy"
      },
      "source": [
        "각 회사마다 세분화하여 감성분석한 결과를 2차원 막대 그래프 형태로 시각화하여 나타낸다. 여기서 x는 회사이고, y는 감성에 해당하는 단어들의 수이며, 막대 그래프 내에 세분화된 감정들이 종류별로 시각화되어 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmMBsHL-zaBz"
      },
      "source": [
        "E=[]\n",
        "plt.rcParams[\"figure.figsize\"]=(20,10)\n",
        "plt.rcParams[\"ytick.labelsize\"]=20\n",
        "plt.rcParams[\"xtick.labelsize\"]=20\n",
        "plt.rcParams[\"legend.fontsize\"]=15\n",
        "\n",
        "for firm in emotion_firm:\n",
        "    E.append(firm[1][2:])\n",
        "a=np.transpose(E)\n",
        "X=np.arange(len(a[0]))\n",
        "plt.bar(X,a[0],label='Anger')\n",
        "plt.bar(X,a[1],bottom=a[0],label='Anticipation')\n",
        "plt.bar(X,a[2],bottom=sum(a[0:2]),label='Disgust')\n",
        "plt.bar(X,a[3],bottom=sum(a[0:3]),label='Fear')\n",
        "plt.bar(X,a[4],bottom=sum(a[0:4]),label='Joy')\n",
        "plt.bar(X,a[5],bottom=sum(a[0:5]),label='Sadness')\n",
        "plt.bar(X,a[6],bottom=sum(a[0:6]),label='Suprise')\n",
        "plt.bar(X,a[7],bottom=sum(a[0:7]),label='Trust')\n",
        "\n",
        "plt.xlabel=\"Firm index\"\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gdiW2CTzaBz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}