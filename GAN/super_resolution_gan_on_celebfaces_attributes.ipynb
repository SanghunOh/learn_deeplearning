{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"super_resolution_gan_on_celebfaces_attributes.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/svgladysh/Super-Resolution-GAN-Experiments/blob/master/super_resolution_gan_on_celebfaces_attributes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"YqQxxfB2ZCQJ","colab_type":"text"},"source":["## Super-Resolution using Generative Adversarial Network\n","\n","### an experiment on \"CelebFaces Attributes\" dataset\n","\n","\n","******************************\n","\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/80.jpeg)\n","\n","************************\n","\n","\n","\n","I have also experimented with Super-Resolution GAN on some other datasets and posted my preliminary results in these Notebooks / Kernels:\n","\n","*****************************\n","\n","#### - Super-Resolution using GAN on Simpsons dataset\n","\n","https://www.kaggle.com/svgladysh/super-resolution-gan-on-simpsons-springfield/\n","\n","*****************************\n","\n","#### - Super-Resolution using GAN on Labeled Faces in the Wild dataset\n","\n","https://www.kaggle.com/svgladysh/super-resolution-gan-on-labeled-faces-in-the-wild\n","\n","******************************"]},{"cell_type":"markdown","metadata":{"id":"VRDYEV6NZCQN","colab_type":"text"},"source":["# References:\n","\n","*************\n","\n","\n","[1] A Deep Journey into Super-resolution: A Survey\n","\n","https://arxiv.org/pdf/1904.07523.pdf\n","\n","\n","******************\n","\n","\n","[2] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial\n","Network \n","\n","https://arxiv.org/pdf/1609.04802.pdf\n","\n","************\n","\n","\n","[3] Lectures from Yandex on Super-Resolution GAN:  \n","\n","\n","https://www.youtube.com/watch?v=tk84ia1K8-E\n","\n","https://www.youtube.com/watch?v=2t05gq13xy0\n","\n","\n","**************\n","\n","[4] Lectures from Moscow Institute of Physics and Technology at Deep Learning School:\n","\n","https://www.dlschool.org/\n","\n","\n","\n","***********************\n","\n","[5] Generative Adversarial Networks Projects by Kailash Ahirwar\n","\n","https://www.amazon.com/Generative-Adversarial-Networks-Projects-next-generation/dp/1789136679\n","\n","https://github.com/PacktPublishing/Generative-Adversarial-Networks-Projects\n","\n","****************************\n","\n","[6] Generative Adversarial Nets \n","\n","https://arxiv.org/pdf/1406.2661.pdf\n","\n","*******************************************************\n","\n","[7] Perceptual Losses for Real-Time Style Transfer and Super-Resolution\n","\n","https://arxiv.org/pdf/1603.08155.pdf\n","\n","*******************************************************\n","\n","[8]  \"CelebFaces Attributes\" dataset\n","\n","http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n","\n","*********************\n","\n","\n","[9]  Deep Residual Learning for Image Recognition \n","\n","https://arxiv.org/pdf/1512.03385.pdf\n","\n","\n","********************\n","\n","[10] Very Deep Convolutional Networks for Large-Scale Image Recognition\n","\n","https://arxiv.org/abs/1409.1556\n","\n","************************\n","\n","[11] ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks \n","\n","https://arxiv.org/pdf/1809.00219.pdf\n","\n","*******************\n","\n","[12] The relativistic discriminator: a key element missing from standard GAN\n","\n","https://arxiv.org/pdf/1807.00734.pdf\n","\n","********************\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sYk607sfZCQO","colab_type":"text"},"source":["## Super-Resolution: Problem Definition\n","\n","\n","Super-resolution is the process of recovering a high-resolution (HR) image from a low-resolution (LR) image. \n","\n","A recovered HR image then is referred as a super-resolution image or SR image. \n","\n","Super-resolution is still considered **a challenging research problem in computer vision**.\n","\n","\n","##### Challenges:\n","\n","* Super-Resolution = ill-posed inverse problem\n","\n","Instead of a single unique solution, there exist multiple solutions for the same low-resolution image. To constrain the solution-space, reliable prior information is typically required. \n","\n","* Up-scaling factor = increase in complexity\n","\n","The complexity of the problem increases as the up-scaling factor increases. At higher factors, the recovery of missing scene details becomes even more complex, and consequently it often leads to reproduction of wrong information. \n","\n","* Assessment of the quality of output\n","\n","Assessment of the quality of output is not straightforward and loosely correlate to human perception.\n","\n","\n","##### Quantitative metrics\n","\n","* PSNR = Peak Signal-to-Noise Ratio\n","PSNR is the ratio between maximum possible power of signal and power of corrupting noise \n","\n","* SSIM = Structural Similarity Index\n","SSIM measures the perceptual difference between two similar images\n","\n","\n","**********************************************\n","\n","### Single Image Super-Resolution\n","\n","* Single Image Super-Resolution (SISR) - generating an up-scaled image from a single source image\n","\n","* Multuple Image Super-Resolution - generating an up-scaled image from multiple source images\n","\n","\n","In this Notebook I am focusing only on Single Image Super-Resolution"]},{"cell_type":"markdown","metadata":{"id":"AFWuIOUjZCQQ","colab_type":"text"},"source":["## Super-Resolution: Methods Classification\n","\n","Super-resolution methods can be categorized into the following taxonomy according to the authors of the survey [1] based on their features: \n","\n","\n","\n","***********************************\n","\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/SR_Taxonomy.png)\n","\n","\n","\n","*******************************************\n","\n","Picture from [1]\n","\n","In this Notebook / Kernel I will be focusing only on Super-Resolution methods **based on GAN models (see the bars in the right side of the diagram above).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2wUSP2sxZCQS","colab_type":"text"},"source":["## GAN \n","\n","Generative Adversarial Networks (GAN) [6] is a Deep Neural Networks architecture based on a game-theoretic approach, where two components of the model, namely a generator and discriminator, try to compete with each other. \n","\n","The Generator is trying to fool the Discriminator by creating faked images. Whereas the Discriminator is trying not to be fooled and learns how to detect faked ones better. In this way the Generator learns to generate better more realistic images.\n","\n","\n","\n","*******************************\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/11.png)\n","\n","\n","Picture from https://mc.ai/review-gan\n","\n","***************************\n","\n","\n","\n","### GAN Applied to the problem of Super-Resolution: \n","\n","The Generator creates SR images that a Discriminator cannot distinguish as a real HR image or an artificially super-resolved output. In this manner, HR images with better perceptual quality are generated.\n","\n","\n","\n","********************************"]},{"cell_type":"markdown","metadata":{"id":"PSe347vkZCQT","colab_type":"text"},"source":["## Loss Functions\n","\n","\n","**Perceptual Loss** is a weighted sum of the content loss and adversarial loss\n","\n","***************************\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/0.png)\n","\n","\n","\n","****************************\n","\n","**Content Loss** can be of two types:\n","\n","         \n","**Pixel-wise MSE** loss mean squared error between each pixel in real image and a pixel in generated image\n","\n","**************************\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/1.png)\n","\n","***************************\n","\n","**VGG loss** is the Euclidean distance between the feature maps of the generated image and the real image\n","\n","\n","***********************\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/2.png)\n","\n","***********************\n","\n","\n","**Adversarial Loss** is calculated based on probabilities provided by Discriminator\n","\n","*************************\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/3.png)\n","\n","**********************\n","\n","**Discriminator** is trained to solve maximization:\n","\n","************************\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/4.png)\n","\n","***********************\n","\n","**Generator** is trained to solve minimization:\n","\n","************************\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/5.png)\n","\n","****************************\n"]},{"cell_type":"markdown","metadata":{"id":"zW53BWRkZCQV","colab_type":"text"},"source":["## SRGAN\n","\n","\n","The authors of SRGAN [2] proposed to use an adversarial objective function that promotes super-resolved (SR) outputs that lie close to the manifold of natural images.\n","\n","The main highlight of their work is a multi-task loss formulation that consists of three main parts: \n","(1) a MSE loss that encodes pixel-wise similarity, \n","(2) a perceptual similarity metric in terms of a distance metric defined over high-level image representation (e.g., deep network features), and \n","(3) an adversarial loss that balances a min-max game between a generator and a discriminator (standard GAN objective [6]). \n","\n","\n","*********************************************\n","\n","\n","#### SRGAN Conceptual architecture \n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/12.jpeg)\n","\n","HR - High Resolution image\n","\n","LR - Low Resolution image\n","\n","SR - Super Resolution image\n","\n","Generator - estimates for a LR its corresponding HR which is a SR\n","\n","Discriminator - is trained to distinguish SR and real images\n","\n","*****************************\n","\n","***********************\n","\n","#### SRGAN Neural network architecture\n","\n"," \n","\n"," \n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/13.jpeg)\n","\n","\n","\n","Picture from [2]\n","\n","*******************************\n","\n","Letters and numbers in the diagram above indicate the following architectural parameters for each convolutional layer in SRGAN:\n","\n","* kernel size (k)  \n","* number of feature maps (n) \n","* stride (s) \n","\n","\n","*****************************"]},{"cell_type":"markdown","metadata":{"id":"eAsSWlahZCQX","colab_type":"text"},"source":["## ESRGAN\n","\n","Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) [11] builds upon SRGAN [2]. ESRGAN main aim is to improve the overall perceptual quality for Super-Resolution. \n","\n","ESRGAN core novelties in comparison with SRGAN:\n","\n","* removed Batch-Norm layers, which proved to increase performance and reduce computational complexity \n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/bn.png)\n","\n","\n","\n","Picture from [11]\n","\n","******************************\n","\n","* introduced Residual in Residual Dense Block, because more layers and connections always boost performance\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/rrdb.png)\n","\n","\n","\n","Picture from [11]\n","\n","************************************\n","\n","* discriminator based on the Relativistic GAN [12],  which tries to predict the probability that a real image is relatively more realistic than a fake one\n","\n","\n","![](https://github.com/svgladysh/Super-Resolution-GAN-Experiments/raw/master/relaGAN.png)\n","\n","\n","\n","Picture from [11]\n","\n","**********************************************\n"]},{"cell_type":"markdown","metadata":{"id":"EGCnuCkGZCQY","colab_type":"text"},"source":["\n","***********************************************\n","\n","# Implementation: initial steps\n","\n","The SRGAN implementation code is based on [5] with several changes made in order to make it work with CelebFaces Attributes (CelebA) Dataset in Kaggle environment, + some extra features being added to play and experiment with, and hyper-parameters being tailored a bit to the current dataset. \n","\n","\n","### Import libraries \n","\n","* Начальные шаги по реализации модели. Импортируем библиотеки"]},{"cell_type":"code","metadata":{"id":"VuLJWGQXZCQa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":98},"outputId":"d3925b09-f32a-4c31-9079-3857670b8fbf","executionInfo":{"status":"ok","timestamp":1584428135862,"user_tz":-540,"elapsed":6558,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["import tensorflow as tf\n","from keras import Input\n","from keras.applications import VGG19, InceptionResNetV2\n","from keras.callbacks import TensorBoard\n","from keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\n","from keras.layers.convolutional import Conv2D, UpSampling2D\n","from keras.models import Model\n","from keras.optimizers import Adam\n","\n","import glob\n","import time\n","import os\n","import cv2\n","import base64\n","import imageio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","from imageio import imread\n","from skimage.transform import resize as imresize\n","from copy import deepcopy\n","from tqdm import tqdm\n","from pprint import pprint\n","from PIL import Image\n","from sklearn.model_selection import train_test_split"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5rKfvkOIZCQk","colab_type":"text"},"source":[" ### Define some  hyper-parameters\n","\n","* Зададим некоторые гиперпараметры"]},{"cell_type":"code","metadata":{"id":"K_qSE3C2ZCQm","colab_type":"code","colab":{}},"source":["# для начала укажем количество эпох равным 100, затем конечно нужно будет продолжить ещё\n","epochs = 2\n","\n","# размер батча выберем равным 8, так как больший размер уже не влезал в оперативную память на Kaggle\n","batch_size = 8\n","\n","# укажем размер изображения с низким разрешением (LR) \n","low_resolution_shape = (64, 64, 3)\n","\n","# укажем размер изображения с высоким разрешением (HR) \n","high_resolution_shape = (256, 256, 3)\n","\n","# для простоты выберем в качестве оптимизатора Adam\n","common_optimizer = Adam(0.0002, 0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DS9hP25Pbx5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"a0a79d61-cc95-4e0e-ca03-bb845185d5a0","executionInfo":{"status":"ok","timestamp":1584428135864,"user_tz":-540,"elapsed":6531,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-xCAtaH0ZCQr","colab_type":"code","colab":{}},"source":["data_dir = \"/content/drive/My Drive/Colab Notebooks/00.NeedToCheck/img_align_celeba/*.*\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USc4oY-lZCQx","colab_type":"text"},"source":["#### CelebA dataset contains images (photos) of people provided with attribute labels  [8]\n","\n","* Датасет CelebA содержит изображения лиц людей с размеченными атрибутами  [8]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KroBJAwjZCQy","colab_type":"text"},"source":["# Generator\n","\n","* Сеть генератор\n","\n","Создадим в качестве \"строительного блока\" - residual block \n","\n","см. архитектура ResNet в статье [9]    https://arxiv.org/pdf/1512.03385.pdf"]},{"cell_type":"code","metadata":{"id":"mabzImSNZCQ0","colab_type":"code","colab":{}},"source":["def residual_block(x):\n","\n","    filters = [64, 64]\n","    #filters = [128, 128]\n","    kernel_size = 3\n","    strides = 1\n","    padding = \"same\"\n","    momentum = 0.8\n","    activation = \"relu\"\n","\n","    res = Conv2D(filters=filters[0], kernel_size=kernel_size, strides=strides, padding=padding)(x)\n","    res = Activation(activation=activation)(res)\n","    res = BatchNormalization(momentum=momentum)(res)\n","\n","    res = Conv2D(filters=filters[1], kernel_size=kernel_size, strides=strides, padding=padding)(res)\n","    res = BatchNormalization(momentum=momentum)(res)\n","\n","    res = Add()([res, x])\n","    return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLkOmovhZCQ5","colab_type":"code","colab":{}},"source":["def build_generator():\n","    \n","    # используем в генераторе 16 residual блоков\n","    residual_blocks = 16\n","    momentum = 0.8\n","    \n","    # размерность соответствует LR - Low Resolution\n","    input_shape = (64, 64, 3)\n","    \n","    # input-слой для сети генератора\n","    input_layer = Input(shape=input_shape)\n","    \n","    # pre-residual block: свёрточный слой перед residual блоками \n","    gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same', activation='relu')(input_layer)\n","    \n","    # добавляем 16 residual блоков\n","    res = residual_block(gen1)\n","    for i in range(residual_blocks - 1):\n","        res = residual_block(res)\n","    \n","    # post-residual block: свёрточный слой и batch-norm слой после residual блоков\n","    gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n","    gen2 = BatchNormalization(momentum=momentum)(gen2)\n","    \n","    # суммируем выходы из pre-residual block(gen1) и the post-residual block(gen2)\n","    gen3 = Add()([gen2, gen1])\n","    \n","    # слой UpSampling: обучаемся повышать размерность\n","    gen4 = UpSampling2D(size=2)(gen3)\n","    gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n","    gen4 = Activation('relu')(gen4)\n","    \n","    # слой UpSampling: обучаемся повышать размерность\n","    gen5 = UpSampling2D(size=2)(gen4)\n","    gen5 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen5)\n","    gen5 = Activation('relu')(gen5)\n","    \n","    # слой convolution на выходе\n","    gen6 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen5)\n","    output = Activation('tanh')(gen6)\n","    \n","    # модель \n","    model = Model(inputs=[input_layer], outputs=[output], name='generator')\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"al6VLlE0ZCQ9","colab_type":"text"},"source":["# Discriminator\n","\n","* Сеть дискриминатор"]},{"cell_type":"code","metadata":{"id":"xk99SzEjZCQ_","colab_type":"code","colab":{}},"source":["def build_discriminator():\n","    \n","    # зададим гипер-параметры\n","    leakyrelu_alpha = 0.2\n","    momentum = 0.8\n","    \n","    # размерность соответствует HR - High Resolution\n","    input_shape = (256, 256, 3)\n","    \n","    # input-слой для сети дискриминатора\n","    input_layer = Input(shape=input_shape)\n","    \n","    # 8 свёрточных слоёв с батч-нормализациями  \n","    dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n","    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n","\n","    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n","    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n","    dis2 = BatchNormalization(momentum=momentum)(dis2)\n","\n","    dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n","    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n","    dis3 = BatchNormalization(momentum=momentum)(dis3)\n","\n","    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n","    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n","    dis4 = BatchNormalization(momentum=0.8)(dis4)\n","\n","    dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n","    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n","    dis5 = BatchNormalization(momentum=momentum)(dis5)\n","\n","    dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n","    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n","    dis6 = BatchNormalization(momentum=momentum)(dis6)\n","\n","    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n","    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n","    dis7 = BatchNormalization(momentum=momentum)(dis7)\n","\n","    dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n","    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n","    dis8 = BatchNormalization(momentum=momentum)(dis8)\n","    \n","    # полносвязный слой \n","    dis9 = Dense(units=1024)(dis8)\n","    dis9 = LeakyReLU(alpha=0.2)(dis9)\n","    \n","    # последний полносвязный слой на выходе - для классификации \n","    output = Dense(units=1, activation='sigmoid')(dis9)\n","    \n","    \n","    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s_g725EuZCRE","colab_type":"text"},"source":["# Pre-trained VGG19 \n","\n","Pre-trained VGG19 will be used for feature extraction from real images and generated images\n","\n","* Предобученная сеть VGG19 для извлечения признаков из реальных и сгенерированных изображений"]},{"cell_type":"code","metadata":{"id":"S_yssq6-ZCRG","colab_type":"code","colab":{}},"source":["def build_vgg():\n","    \n","    # размерность соответствует HR - High Resolution\n","    input_shape = (256, 256, 3)\n","    \n","    # загружаем VGG19 предобученную на датасете 'Imagenet'\n","    vgg = VGG19(weights=\"imagenet\")\n","    \n","    # возьмём выход с 9-го слоя\n","    vgg.outputs = [vgg.layers[9].output]\n","    \n","    # зададим входной слой\n","    input_layer = Input(shape=input_shape)\n","    \n","    # извлечём признаки \n","    features = vgg(input_layer)\n","    \n","    # модель\n","    model = Model(inputs=[input_layer], outputs=[features])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUzq9gf1ZCRO","colab_type":"text"},"source":["Let's consider pre-trained InceptionResNetV2, ResNet50, ResNet152V2 as possible alternatives, which could be compared with VGG19\n","\n","* Рассмотрим предобученные InceptionResNetV2, ResNet50, ResNet152V2 в качестве возможных альтернатив, с которыми мы сможем сравнивать VGG19"]},{"cell_type":"code","metadata":{"id":"smM-WEklZCRP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"7468e03c-e462-4a8a-d159-ed4191f07b61","executionInfo":{"status":"ok","timestamp":1584428135868,"user_tz":-540,"elapsed":6462,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\"\"\"\n","def build_InceptionResNetV2():\n","    input_shape = (256, 256, 3)\n","    \n","    resnetV2 = InceptionResNetV2(weights='imagenet')\n","    resnetV2.outputs = [resnetV2.layers[-1].output]\n","    \n","    input_layer = Input(shape=input_shape)\n","    \n","    features = resnetV2(input_layer)\n","    \n","    model = Model(inputs=[input_layer], outputs=[features])\n","    return model\n","\"\"\""],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef build_InceptionResNetV2():\\n    input_shape = (256, 256, 3)\\n    \\n    resnetV2 = InceptionResNetV2(weights='imagenet')\\n    resnetV2.outputs = [resnetV2.layers[-1].output]\\n    \\n    input_layer = Input(shape=input_shape)\\n    \\n    features = resnetV2(input_layer)\\n    \\n    model = Model(inputs=[input_layer], outputs=[features])\\n    return model\\n\""]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"9Qb-w3NAZCRU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"372cdbe9-1e98-4cc0-efe1-2cfe9e72bc25","executionInfo":{"status":"ok","timestamp":1584428135869,"user_tz":-540,"elapsed":6445,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\"\"\"\n","def build_ResNet50():\n","    input_shape = (256, 256, 3)\n","    \n","    resent50 = ResNet50(weights='imagenet', input_shape=input_shape)\n","    resnet50.outputs = [resent50.layers[-1].output]\n","    \n","    input_layer = Input(shape=input_shape)\n","    \n","    features = resnet50(input_layer)\n","    \n","    model = Model(inputs=[input_layer], outputs=[features])\n","    return model\n","\"\"\""],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef build_ResNet50():\\n    input_shape = (256, 256, 3)\\n    \\n    resent50 = ResNet50(weights='imagenet', input_shape=input_shape)\\n    resnet50.outputs = [resent50.layers[-1].output]\\n    \\n    input_layer = Input(shape=input_shape)\\n    \\n    features = resnet50(input_layer)\\n    \\n    model = Model(inputs=[input_layer], outputs=[features])\\n    return model\\n\""]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"bTLE1BedZCRY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"cee3bb34-f66f-467e-8e2d-3b905e846e0b","executionInfo":{"status":"ok","timestamp":1584428135870,"user_tz":-540,"elapsed":6427,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\"\"\"\n","def build_ResNet152V2():\n","    input_shape = (256, 256, 3)\n","    \n","    resent152 = InceptionResNetV2(weights='imagenet', input_shape=input_shape)\n","    resnet152.outputs = [resent152.layers[-1].output]\n","    \n","    input_layer = Input(shape=input_shape)\n","    \n","    features = resnet152(input_layer)\n","    \n","    model = Model(inputs=[input_layer], outputs=[features])\n","    return model\n","\"\"\""],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef build_ResNet152V2():\\n    input_shape = (256, 256, 3)\\n    \\n    resent152 = InceptionResNetV2(weights='imagenet', input_shape=input_shape)\\n    resnet152.outputs = [resent152.layers[-1].output]\\n    \\n    input_layer = Input(shape=input_shape)\\n    \\n    features = resnet152(input_layer)\\n    \\n    model = Model(inputs=[input_layer], outputs=[features])\\n    return model\\n\""]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"6WL7WYPwZCRd","colab_type":"text"},"source":["## Sampling images\n","\n","* Реализуем функцию для сэмплирования изображений"]},{"cell_type":"code","metadata":{"id":"WcyNbpcfZCRe","colab_type":"code","colab":{}},"source":["def sample_images(data_dir, batch_size, high_resolution_shape, low_resolution_shape):\n","    \n","    # создадим список всех изображений, находящихся внутри каталога data_dir\n","    all_images = glob.glob(data_dir)\n","    \n","    # выберем случайный батч с изображениями\n","    images_batch = np.random.choice(all_images, size=batch_size)\n","\n","    low_resolution_images = []\n","    high_resolution_images = []\n","\n","    for img in images_batch:\n","        # получим numpy ndarray текущего изображения\n","        img1 = imread(img, as_gray=False, pilmode='RGB')\n","        img1 = img1.astype(np.float32)\n","        \n","        # изменим размеры\n","        img1_high_resolution = imresize(img1, high_resolution_shape)\n","        img1_low_resolution = imresize(img1, low_resolution_shape)\n","        \n","        # применим аугментацию: random horizontal flip\n","        if np.random.random() < 0.5:\n","            img1_high_resolution = np.fliplr(img1_high_resolution)\n","            img1_low_resolution = np.fliplr(img1_low_resolution)\n","\n","        high_resolution_images.append(img1_high_resolution)\n","        low_resolution_images.append(img1_low_resolution)\n","    \n","    # конвертируем списки в numpy ndarrays\n","    return np.array(high_resolution_images), np.array(low_resolution_images)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bs4s5yGMZCRj","colab_type":"text"},"source":["## Saving images\n","\n","* Реализуем функцию для сохранения изображений"]},{"cell_type":"code","metadata":{"id":"J-P-onkwZCRl","colab_type":"code","colab":{}},"source":["def save_images(low_resolution_image, original_image, generated_image, path):\n","\n","    # сохраним low-resolution, high-resolution(original) и generated high-resolution изображения в одной картинке\n","\n","    fig = plt.figure()\n","    \n","    ax = fig.add_subplot(1, 3, 1)\n","    ax.imshow(original_image)\n","    ax.axis(\"off\")\n","    ax.set_title(\"ORIGINAL\")\n","    \n","    ax = fig.add_subplot(1, 3, 2)\n","    ax.imshow(low_resolution_image)\n","    ax.axis(\"off\")\n","    ax.set_title(\"LOW_RESOLUTION\")\n","\n","    ax = fig.add_subplot(1, 3, 3)\n","    ax.imshow(generated_image)\n","    ax.axis(\"off\")\n","    ax.set_title(\"GENERATED\")\n","\n","    plt.savefig(path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2qr5_PboZCRp","colab_type":"text"},"source":["## VGG19 compilation\n","\n","* Скомпилируем предобученную сеть VGG19"]},{"cell_type":"code","metadata":{"id":"lYcZ_Ns7ZCRq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":635},"outputId":"a4ef238c-a2f2-4fa5-a3f4-a8255c04c570","executionInfo":{"status":"ok","timestamp":1584428149031,"user_tz":-540,"elapsed":19552,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["vgg = build_vgg()\n","vgg.trainable = False\n","vgg.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n","vgg.summary()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 256, 256, 3)       0         \n","_________________________________________________________________\n","vgg19 (Model)                multiple                  143667240 \n","=================================================================\n","Total params: 143,667,240\n","Trainable params: 0\n","Non-trainable params: 143,667,240\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oXASNNHMZCRu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"d4c7e643-24a3-44f6-cf46-2d0869c2a7be","executionInfo":{"status":"ok","timestamp":1584428149032,"user_tz":-540,"elapsed":19535,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\"\"\"\n","vgg = build_InceptionResNetV2()\n","vgg.trainable = False\n","vgg.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n","\"\"\""],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nvgg = build_InceptionResNetV2()\\nvgg.trainable = False\\nvgg.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\\n\""]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"NY15MQzAZCR0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"84274841-bdb2-46cf-9de4-940ec6482d82","executionInfo":{"status":"ok","timestamp":1584428149033,"user_tz":-540,"elapsed":19518,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\"\"\"\n","resnet50 = build_ResNet50()\n","resnet50.trainable = False\n","resnet50.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n","\"\"\""],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nresnet50 = build_ResNet50()\\nresnet50.trainable = False\\nresnet50.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\\n\""]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"zUcd8zSbZCR5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"ebd6d585-e637-4f13-a6a5-d4d4c1972443","executionInfo":{"status":"ok","timestamp":1584428149034,"user_tz":-540,"elapsed":19503,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\"\"\"\n","resnet152 = build_ResNet152V2()\n","resnet152.trainable = False\n","resnet152.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n","\"\"\""],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nresnet152 = build_ResNet152V2()\\nresnet152.trainable = False\\nresnet152.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\\n\""]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"0FyRufplZCR9","colab_type":"text"},"source":["## Discriminator compilation\n","\n","* Скомпилируем сеть дискриминатор"]},{"cell_type":"code","metadata":{"id":"aM0wvP1HZCR_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"9572daa8-77e3-4d33-e14b-179d5843dfa9","executionInfo":{"status":"ok","timestamp":1584428149961,"user_tz":-540,"elapsed":20417,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\n","discriminator = build_discriminator()\n","discriminator.trainable = True\n","discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4VIMeDiZZCSD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"171d7d7e-a228-4219-ba95-067f8ff02f77","executionInfo":{"status":"ok","timestamp":1584428149962,"user_tz":-540,"elapsed":20405,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["\"\"\"\n","discriminator = build_discriminator()\n","discriminator.trainable = False\n","discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n","\"\"\""],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndiscriminator = build_discriminator()\\ndiscriminator.trainable = False\\ndiscriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\\n\""]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"7cOwO5lxZCSG","colab_type":"text"},"source":["## Generator build\n","\n","\n","* Сделаем build генератора\n"]},{"cell_type":"code","metadata":{"id":"Pnf9d_C_ZCSI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"1401cdf6-40dc-44f8-89a3-0747bef35845","executionInfo":{"status":"ok","timestamp":1584428152596,"user_tz":-540,"elapsed":23021,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["generator = build_generator()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1jcnPqBzZCSM","colab_type":"text"},"source":["## Adversarial model compilation\n","\n","* Скомпилируем созтязательную модель, которая будет включать в себя генератор, дискриминатор и предобученную сеть VGG19"]},{"cell_type":"code","metadata":{"id":"aX7a6dtcZCSN","colab_type":"code","colab":{}},"source":["def build_adversarial_model(generator, discriminator, vgg):\n","    \n","    # входной слой для high-resolution изображений\n","    input_high_resolution = Input(shape=high_resolution_shape)\n","\n","    # входной слой для low-resolution изображений\n","    input_low_resolution = Input(shape=low_resolution_shape)\n","\n","    # сгенерируем high-resolution изображения из low-resolution изображений\n","    generated_high_resolution_images = generator(input_low_resolution)\n","\n","    # извлечём feature maps из generated images\n","    features = vgg(generated_high_resolution_images)\n","    \n","    # Сделаем здесь внутри GAN дискриминатор необучаемым, потому что \n","    # в состязательной сети нам не нужно обучать дискриминатор в то время, когда обучается генератор.\n","    discriminator.trainable = False\n","    discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n","\n","    # дискриминатор даст нам оценку вероятностей generated high-resolution изображений\n","    probs = discriminator(generated_high_resolution_images)\n","\n","    # создадим и скомпилируем сотязательную модель\n","    adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n","    adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n","    return adversarial_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1QmIAMpZCSS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"b57851bc-a8ba-433f-e41f-a1f76e7d1aac","executionInfo":{"status":"ok","timestamp":1584428155421,"user_tz":-540,"elapsed":25826,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["adversarial_model = build_adversarial_model(generator, discriminator, vgg)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SzdhcFWTZCSV","colab_type":"text"},"source":["# Training loop on CelebA dataset\n","\n","* Запустим цикл обучения на датасете CelebA\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zyoo6b5TZCSW","colab_type":"text"},"source":["Будем обучать SRGAN в 2 этапа: \n","* сначала на первом этапе обучаем дискриминатор\n","* затем на втором этапе обучаем состязательную сеть, внутри которой у нас обучается генератор, но заморожен дискриминатор\n","* ... да, вот так по-хитрому! "]},{"cell_type":"code","metadata":{"id":"UDWAa6WIZCSX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"05bbb3e0-10ef-4ee0-dfb6-4075074379e1"},"source":["for epoch in range(epochs):\n","    # обучаем дискриминатор\n","    d_history = []\n","    g_history = []\n","    print(\"Epoch:{}\".format(epoch))\n","    \n","    # сэмплируем батч с изображениями\n","    high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n","                                                                          low_resolution_shape=low_resolution_shape,\n","                                                                          high_resolution_shape=high_resolution_shape)\n","    \n","    # нормализуем изображения\n","    high_resolution_images = high_resolution_images / 127.5 - 1.\n","    low_resolution_images = low_resolution_images / 127.5 - 1.\n","    \n","    # сгенерируем high-resolution изображения из low-resolution изображений\n","    generated_high_resolution_images = generator.predict(low_resolution_images)\n","    \n","    # сгенерируем батч настоящих и поддельных меток\n","    real_labels = np.ones((batch_size, 16, 16, 1))\n","    fake_labels = np.zeros((batch_size, 16, 16, 1))\n","    \n","    # обучим дискриминатор на настоящих и поддельных изображениях\n","    d_loss_real = discriminator.train_on_batch(high_resolution_images, real_labels)\n","    d_loss_real =  np.mean(d_loss_real)\n","    d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n","    d_loss_fake =  np.mean(d_loss_fake)\n","    # посчитаем общий loss дискриминатора как среднее арифметическое потерь на настоящих и на поддельных метках\n","    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","    d_history.append(d_loss)\n","    print(\"D_loss:\", d_loss)\n","    \n","    \n","    # обучаем генератор\n","    \n","    # сэмплируем батч с изображениями\n","    high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n","                                                                    low_resolution_shape=low_resolution_shape,\n","                                                                    high_resolution_shape=high_resolution_shape)\n","    \n","    #  нормализуем изображения\n","    high_resolution_images = high_resolution_images / 127.5 - 1.\n","    low_resolution_images = low_resolution_images / 127.5 - 1.\n","    \n","    # извлечём feature maps для настоящих high-resolution изображений\n","    image_features = vgg.predict(high_resolution_images)\n","    \n","    # обучим генератор\n","    g_loss = adversarial_model.train_on_batch([low_resolution_images, high_resolution_images],\n","                                             [real_labels, image_features])\n","    g_history.append( 0.5 * (g_loss[1]) )\n","    print( \"G_loss:\", 0.5 * (g_loss[1]) )\n","    \n","    # сохраним и выведем сэмплы изображений после каждых 10 эпох\n","    if epoch % 20 == 0:\n","        high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n","                                                                        low_resolution_shape=low_resolution_shape,\n","                                                                        high_resolution_shape=high_resolution_shape)\n","        \n","        # нормализуем изображения\n","        high_resolution_images = high_resolution_images / 127.5 - 1.\n","        low_resolution_images = low_resolution_images / 127.5 - 1.\n","\n","        generated_images = generator.predict_on_batch(low_resolution_images)\n","\n","        for index, img in enumerate(generated_images):\n","            save_images(low_resolution_images[index], high_resolution_images[index], img,\n","                        path=\"img_{}_{}\".format(epoch, index))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch:0\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","D_loss: 0.38818633556365967\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RfbZhMEmZCSc","colab_type":"text"},"source":["#### Промежуточная валидация результатов\n","\n","Посмотрим глазами на эти тройки изображений:\n","ORIGINAL -- LOW RESOLUTION -- GENERATED\n","\n","Провалидируем результат, для того чтобы понять, нужно ли продолжать обучение ещё некоторое количество эпох.\n","Если сгенерились хорошие картинки, то можно прекращать."]},{"cell_type":"code","metadata":{"id":"1bHldF1JZCSe","colab_type":"code","colab":{}},"source":["for epoch in range(epochs):\n","    # обучаем дискриминатор\n","    d_history = []\n","    g_history = []\n","    print(\"Epoch:{}\".format(epoch))\n","    \n","    # сэмплируем батч с изображениями\n","    high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n","                                                                          low_resolution_shape=low_resolution_shape,\n","                                                                          high_resolution_shape=high_resolution_shape)\n","    \n","    # нормализуем изображения\n","    high_resolution_images = high_resolution_images / 127.5 - 1.\n","    low_resolution_images = low_resolution_images / 127.5 - 1.\n","    \n","    # сгенерируем high-resolution изображения из low-resolution изображений\n","    generated_high_resolution_images = generator.predict(low_resolution_images)\n","    \n","    # сгенерируем батч настоящих и поддельных меток\n","    real_labels = np.ones((batch_size, 16, 16, 1))\n","    fake_labels = np.zeros((batch_size, 16, 16, 1))\n","    \n","    # обучим дискриминатор на настоящих и поддельных изображениях\n","    d_loss_real = discriminator.train_on_batch(high_resolution_images, real_labels)\n","    d_loss_real =  np.mean(d_loss_real)\n","    d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n","    d_loss_fake =  np.mean(d_loss_fake)\n","    # посчитаем общий loss дискриминатора как среднее арифметическое потерь на настоящих и на поддельных метках\n","    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","    d_history.append(d_loss)\n","    print(\"D_loss:\", d_loss)\n","    \n","    \n","    # обучаем генератор\n","    \n","    # сэмплируем батч с изображениями\n","    high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n","                                                                    low_resolution_shape=low_resolution_shape,\n","                                                                    high_resolution_shape=high_resolution_shape)\n","    \n","    #  нормализуем изображения\n","    high_resolution_images = high_resolution_images / 127.5 - 1.\n","    low_resolution_images = low_resolution_images / 127.5 - 1.\n","    \n","    # извлечём feature maps для настоящих high-resolution изображений\n","    image_features = vgg.predict(high_resolution_images)\n","    \n","    # обучим генератор\n","    g_loss = adversarial_model.train_on_batch([low_resolution_images, high_resolution_images],\n","                                             [real_labels, image_features])\n","    g_history.append( 0.5 * (g_loss[1]) )\n","    print( \"G_loss:\", 0.5 * (g_loss[1]) )\n","    \n","    # сохраним и выведем сэмплы изображений после каждых 10 эпох\n","    if epoch % 20 == 0:\n","        high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,\n","                                                                        low_resolution_shape=low_resolution_shape,\n","                                                                        high_resolution_shape=high_resolution_shape)\n","        \n","        # нормализуем изображения\n","        high_resolution_images = high_resolution_images / 127.5 - 1.\n","        low_resolution_images = low_resolution_images / 127.5 - 1.\n","\n","        generated_images = generator.predict_on_batch(low_resolution_images)\n","\n","        for index, img in enumerate(generated_images):\n","            save_images(low_resolution_images[index], high_resolution_images[index], img,\n","                        path=\"/kaggle/working/img_{}_{}\".format(epoch, index))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2HzJZcdZCSj","colab_type":"text"},"source":["## Save models weights\n","\n","* Сохраним веса моделей"]},{"cell_type":"code","metadata":{"id":"LZCHgCMPZCSk","colab_type":"code","colab":{}},"source":["generator.save_weights(\"/kaggle/working/generator.h5\")\n","discriminator.save_weights(\"/kaggle/working/discriminator.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8IgEcByZCSq","colab_type":"text"},"source":["# Eval mode\n","\n","* Режим предсказания"]},{"cell_type":"code","metadata":{"id":"s2Y1S3ImZCSs","colab_type":"code","colab":{}},"source":["#discriminator = build_discriminator()\n","#generator = build_generator()\n","\n","generator.load_weights(\"/kaggle/working/generator.h5\")\n","discriminator.load_weights(\"/kaggle/working/discriminator.h5\")\n","\n","high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=10,\n","                                                                      low_resolution_shape=low_resolution_shape,\n","                                                                      high_resolution_shape=high_resolution_shape)\n","\n","high_resolution_images = high_resolution_images / 127.5 - 1.\n","low_resolution_images = low_resolution_images / 127.5 - 1.\n","\n","generated_images = generator.predict_on_batch(low_resolution_images)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"PdTLH0bfZCSv","colab_type":"text"},"source":["## Save images\n","\n","* Сохраняем изображения"]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"aUEQEmjZZCSw","colab_type":"code","colab":{}},"source":["for index, img in enumerate(generated_images):\n","    save_images(low_resolution_images[index], high_resolution_images[index], img,\n","                path=\"/kaggle/working/gen_{}\".format(index))"],"execution_count":0,"outputs":[]}]}