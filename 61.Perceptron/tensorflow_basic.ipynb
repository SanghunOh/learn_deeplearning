{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"00_everybody_deeplearning_code.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IVOTdElHQUxJ","colab_type":"text"},"source":["# Tensor Manipulation"]},{"cell_type":"code","metadata":{"id":"yH8Fex7PQUxO","colab_type":"code","colab":{}},"source":["# https://www.tensorflow.org/api_guides/python/array_ops\n","import tensorflow as tf\n","import numpy as np\n","import pprint\n","tf.set_random_seed(777)  # for reproducibility\n","\n","pp = pprint.PrettyPrinter(indent=4)\n","sess = tf.InteractiveSession()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxV002lWQUxX","colab_type":"text"},"source":["## Simple Array"]},{"cell_type":"code","metadata":{"id":"Zv3nXyxtQUxZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":239},"outputId":"e0670ca7-a452-4ff8-c0bd-d44508d52070","executionInfo":{"status":"aborted","timestamp":1583220043201,"user_tz":-540,"elapsed":11244,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["t = np.array([0., 1., 2., 3., 4., 5., 6.])\n","pp.pprint(t)\n","print(t.ndim) # rank\n","print(t.shape) # shape\n","print(t[0], t[1], t[-1])\n","print(t[2:5], t[4:-1])\n","print(t[:2], t[3:])"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cdd8a7140b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"fzpVXxcqQUxh","colab_type":"text"},"source":["# 2D Array"]},{"cell_type":"code","metadata":{"id":"XM32FvInQUxj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":222},"outputId":"8c561413-8841-49bd-965d-086c20077b72","executionInfo":{"status":"aborted","timestamp":1583220043201,"user_tz":-540,"elapsed":11217,"user":{"displayName":"sanghun oh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlx86owSfLI1ysBzewc-4sBRVA0uQ32rdNEjLr1Q=s64","userId":"06412681606171338922"}}},"source":["t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n","pp.pprint(t)\n","print(t.ndim) # rank\n","print(t.shape) # shape"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-efbc37b43307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"OHSssPj3QUxp","colab_type":"text"},"source":["## Shape, Rank, Axis"]},{"cell_type":"code","metadata":{"id":"ohIEphSRQUxr","colab_type":"code","colab":{}},"source":["t = tf.constant([1,2,3,4])\n","tf.shape(t).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXl3YkOgQUxx","colab_type":"code","colab":{}},"source":["t = tf.constant([[1,2],\n","                 [3,4]])\n","tf.shape(t).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0y7O46M3QUx4","colab_type":"code","colab":{}},"source":["t = tf.constant([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],[[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n","tf.shape(t).eval()\n","# print(t)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNwBA4BvQUx-","colab_type":"code","colab":{}},"source":["[\n","    [\n","        [\n","            [1,2,3,4], \n","            [5,6,7,8],\n","            [9,10,11,12]\n","        ],\n","        [\n","            [13,14,15,16],\n","            [17,18,19,20], \n","            [21,22,23,24]\n","        ]\n","    ]\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N17ICZuNQUyE","colab_type":"text"},"source":["## Matmul VS multiply"]},{"cell_type":"code","metadata":{"id":"ACFpZNrtQUyF","colab_type":"code","colab":{}},"source":["matrix1 = tf.constant([[3., 3.]])\n","matrix2 = tf.constant([[2.],[2.]])\n","tf.matmul(matrix1, matrix2).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E43pVGsjQUyL","colab_type":"code","colab":{}},"source":["(matrix1*matrix2).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFJbUMurQUyR","colab_type":"text"},"source":["## Watch out broadcasting"]},{"cell_type":"code","metadata":{"id":"xEx2mJGJQUyT","colab_type":"code","colab":{}},"source":["matrix1 = tf.constant([[3., 3.]])\n","matrix2 = tf.constant([[2.],[2.]])\n","(matrix1+matrix2).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1x6MzI7nQUyY","colab_type":"code","colab":{}},"source":["matrix1 = tf.constant([[3., 3.]])\n","matrix2 = tf.constant([[2., 2.]])\n","(matrix1+matrix2).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnN0JiTnQUye","colab_type":"text"},"source":["## Random values for variable initializations "]},{"cell_type":"code","metadata":{"scrolled":true,"id":"goyGXEWBQUyg","colab_type":"code","colab":{}},"source":["tf.random_normal([3]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"50LtFcjKQUyl","colab_type":"code","colab":{}},"source":["array = tf.random_normal([100]).eval()\n","import matplotlib.pyplot as plt\n","plt.plot(array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"jVAnwsa6QUys","colab_type":"code","colab":{}},"source":["array = tf.random_normal([100]).eval()\n","import matplotlib.pyplot as plt\n","plt.hist(array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_grU5IwwQUyy","colab_type":"code","colab":{}},"source":["tf.random_uniform([2]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrkrA3znQUy3","colab_type":"code","colab":{}},"source":["array = tf.random_uniform([100]).eval()\n","plt.plot(array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILgCCcj-QUy8","colab_type":"code","colab":{}},"source":["tf.random_uniform([2, 3]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mCUi_8D4QUzB","colab_type":"text"},"source":["## Reduce Mean/Sum"]},{"cell_type":"code","metadata":{"id":"6Cz_J-dCQUzC","colab_type":"code","colab":{}},"source":["tf.reduce_mean([1, 2], axis=0).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D5_74DRTQUzJ","colab_type":"code","colab":{}},"source":["x = [[1., 2.],\n","     [3., 4.]]\n","\n","\n","tf.reduce_mean(x).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lA1rokYEQUzO","colab_type":"code","colab":{}},"source":["tf.reduce_mean(x, axis=0).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HC7ggFs2QUzU","colab_type":"code","colab":{}},"source":["tf.reduce_mean(x, axis=1).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sd3nXbqeQUzY","colab_type":"code","colab":{}},"source":["tf.reduce_mean(x, axis=-1).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVJJpx9KQUzd","colab_type":"code","colab":{}},"source":["tf.reduce_sum(x).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6ciw0MjQUzi","colab_type":"code","colab":{}},"source":["tf.shape(x).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiRQLbSyQUzn","colab_type":"code","colab":{}},"source":["tf.reduce_sum(x, axis=0).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8z38uhDQUzs","colab_type":"code","colab":{}},"source":["tf.reduce_sum(x, axis=-1).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4JFn6SEQUzz","colab_type":"code","colab":{}},"source":["tf.reduce_mean(tf.reduce_sum(x, axis=-1)).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-U15cJgQUz5","colab_type":"text"},"source":["## Argmax with axis"]},{"cell_type":"code","metadata":{"id":"3cbQVvHSQUz7","colab_type":"code","colab":{}},"source":["x = [[0, 1, 2],\n","     [2, 1, 0]]\n","tf.argmax(x, axis=0).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUaeYLCZQU0A","colab_type":"code","colab":{}},"source":["tf.shape(x).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4ItM5OaQU0G","colab_type":"code","colab":{}},"source":["tf.argmax(x, axis=1).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-_6Sc43QU0K","colab_type":"code","colab":{}},"source":["tf.argmax(x, axis=-1).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jXDXm7AQU0P","colab_type":"text"},"source":["## Reshape, squeeze, expand_dims"]},{"cell_type":"code","metadata":{"id":"8YGohjiEQU0Q","colab_type":"code","colab":{}},"source":["t = np.array([[[0, 1, 2], \n","               [3, 4, 5]],\n","              \n","              [[6, 7, 8], \n","               [9, 10, 11]]])\n","t.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVn6nminQU0X","colab_type":"code","colab":{}},"source":["tf.reshape(t, shape=[-1, 3]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vle3EPPEQU0b","colab_type":"code","colab":{}},"source":["tf.reshape(t, shape=[-1, 1, 3]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bntcWzNSQU0g","colab_type":"code","colab":{}},"source":["tf.squeeze([[0], [1], [2]]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljq-8yYvQU0k","colab_type":"code","colab":{}},"source":["tf.expand_dims([0, 1, 2], 1).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c1L9M8vQQU0o","colab_type":"text"},"source":["## One hot"]},{"cell_type":"code","metadata":{"id":"6n3Pi_gqQU0p","colab_type":"code","colab":{}},"source":["tf.one_hot([[0], [1], [2], [0]], depth=3).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8TCQvTjQU0v","colab_type":"code","colab":{}},"source":["t = tf.one_hot([[0], [1], [2], [0]], depth=3) # one-hot을 적용하면 차원이 추가됨 - > 차원을 줄여주어야.\n","tf.reshape(t, shape=[-1, 3]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w7yPi63-QU00","colab_type":"text"},"source":["## casting"]},{"cell_type":"code","metadata":{"id":"zXugG9ZlQU03","colab_type":"code","colab":{}},"source":["tf.cast([1.8, 2.2, 3.3, 4.9], tf.int32).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmMzHmfZQU07","colab_type":"code","colab":{}},"source":["tf.cast([1, 2, 3, 4], tf.float32).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4X138ZcQU0_","colab_type":"code","colab":{}},"source":["tf.cast([True, False, 1 == 1, 0 == 1], tf.int32).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9aPpcV8TQU1E","colab_type":"text"},"source":["## Stack"]},{"cell_type":"code","metadata":{"id":"PqmCxNvBQU1I","colab_type":"code","colab":{}},"source":["x = [1, 4]\n","y = [2, 5]\n","z = [3, 6]\n","\n","# Pack along first dim.\n","tf.stack([x, y, z]).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDfFKgzQQU1M","colab_type":"code","colab":{}},"source":["tf.stack([x, y, z], axis=1).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SgMRfkiQU1R","colab_type":"text"},"source":["## Ones like and Zeros like"]},{"cell_type":"code","metadata":{"id":"tAF7zGX3QU1S","colab_type":"code","colab":{}},"source":["x = [[0, 1, 2],\n","     [2, 1, 0]]\n","\n","tf.ones_like(x).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c67RQ4oZQU1W","colab_type":"code","colab":{}},"source":["tf.zeros_like(x).eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwMiAsv3QU1a","colab_type":"text"},"source":["## Zip\n"]},{"cell_type":"code","metadata":{"id":"CBbsOQ5LQU1b","colab_type":"code","colab":{}},"source":["for x, y in zip([1, 2, 3], [4, 5, 6]):\n","    print(x, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbRj0AKtQU1f","colab_type":"code","colab":{}},"source":["for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n","    print(x, y, z)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hggdP1mdQU1j","colab_type":"text"},"source":["## Transpose"]},{"cell_type":"code","metadata":{"id":"YKXjNi-0QU1k","colab_type":"code","colab":{}},"source":["t = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]])\n","pp.pprint(t.shape)\n","pp.pprint(t)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PvE5KP1QU1o","colab_type":"code","colab":{}},"source":["t1 = tf.transpose(t, [1, 0, 2])\n","pp.pprint(sess.run(t1).shape)\n","pp.pprint(sess.run(t1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwfdUqvzQU1s","colab_type":"code","colab":{}},"source":["t = tf.transpose(t1, [1, 0, 2])\n","pp.pprint(sess.run(t).shape)\n","pp.pprint(sess.run(t))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6GKpdfAaQU1v","colab_type":"code","colab":{}},"source":["t2 = tf.transpose(t, [1, 2, 0])\n","pp.pprint(sess.run(t2).shape)\n","pp.pprint(sess.run(t2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4RITYSwQU1y","colab_type":"code","colab":{}},"source":["t = tf.transpose(t2, [2, 0, 1])\n","pp.pprint(sess.run(t).shape)\n","pp.pprint(sess.run(t))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4xdEUETQU12","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fmn1TSnKQU15","colab_type":"code","colab":{}},"source":["# Lab 9 XOR - Logistic Regression\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.set_random_seed(777)  # for reproducibility\n","learning_rate = 0.1\n","\n","x_data = [[0, 0],\n","          [0, 1],\n","          [1, 0],\n","          [1, 1]]\n","y_data = [[0],\n","          [1],\n","          [1],\n","          [0]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","X = tf.placeholder(tf.float32, [None, 2])\n","Y = tf.placeholder(tf.float32, [None, 1])\n","\n","W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n","hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n","\n","# cost/loss function\n","cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n","                       tf.log(1 - hypothesis))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# Accuracy computation\n","# True if hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(10001):\n","        sess.run(train, feed_dict={X: x_data, Y: y_data})\n","        if step % 100 == 0:\n","            print(step, sess.run(cost, feed_dict={\n","                  X: x_data, Y: y_data}), sess.run(W))\n","\n","    # Accuracy report\n","    h, c, a = sess.run([hypothesis, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3YdaGFOQU18","colab_type":"code","colab":{}},"source":["# Lab 9 XOR - NN\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.set_random_seed(777)  # for reproducibility\n","learning_rate = 0.1\n","\n","x_data = [[0, 0],\n","          [0, 1],\n","          [1, 0],\n","          [1, 1]]\n","y_data = [[0],\n","          [1],\n","          [1],\n","          [0]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","X = tf.placeholder(tf.float32, [None, 2])\n","Y = tf.placeholder(tf.float32, [None, 1])\n","\n","W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1') # 가중치를 초기에 random하게 준다\n","b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n","layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","\n","W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n","b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n","hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n","\n","# cost/loss function\n","cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n","                       tf.log(1 - hypothesis))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# Accuracy computation\n","# True if hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(10001):\n","        sess.run(train, feed_dict={X: x_data, Y: y_data})\n","        if step % 100 == 0:\n","            print(step, sess.run(cost, feed_dict={\n","                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n","\n","    # Accuracy report - 학습데이터를 그대로 사용\n","    h, c, a = sess.run([hypothesis, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nPredicted: \", c, \"\\nAccuracy: \", a)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxi595INQU2C","colab_type":"code","colab":{}},"source":["# Lab 9 XOR\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.set_random_seed(777)  # for reproducibility\n","learning_rate = 0.1\n","\n","x_data = [[0, 0],\n","          [0, 1],\n","          [1, 0],\n","          [1, 1]]\n","y_data = [[0],\n","          [1],\n","          [1],\n","          [0]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","X = tf.placeholder(tf.float32, [None, 2])\n","Y = tf.placeholder(tf.float32, [None, 1])\n","\n","W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n","b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n","layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","\n","W2 = tf.Variable(tf.random_normal([10, 1]), name='weight2')\n","b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n","hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n","\n","\n","# cost/loss function\n","cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n","                       tf.log(1 - hypothesis))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# Accuracy computation\n","# True if hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(10001):\n","        sess.run(train, feed_dict={X: x_data, Y: y_data})\n","        if step % 100 == 0:\n","            print(step, sess.run(cost, feed_dict={\n","                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n","\n","    # Accuracy report\n","    h, c, a = sess.run([hypothesis, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wg4mWVRRQU2F","colab_type":"code","colab":{}},"source":["# Lab 9 XOR - Deep & Wide\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.set_random_seed(777)  # for reproducibility\n","learning_rate = 0.1\n","\n","x_data = [[0, 0],\n","          [0, 1],\n","          [1, 0],\n","          [1, 1]]\n","y_data = [[0],\n","          [1],\n","          [1],\n","          [0]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","X = tf.placeholder(tf.float32, [None, 2])\n","Y = tf.placeholder(tf.float32, [None, 1])\n","\n","W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')\n","b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n","layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","\n","W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n","b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n","layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n","\n","W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n","b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n","layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n","\n","W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n","b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n","hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n","\n","# cost/loss function\n","cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n","                       tf.log(1 - hypothesis))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# Accuracy computation\n","# True if hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(10001):\n","        sess.run(train, feed_dict={X: x_data, Y: y_data})\n","        if step % 100 == 0:\n","            print(step, sess.run(cost, feed_dict={\n","                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n","\n","    # Accuracy report\n","    h, c, a = sess.run([hypothesis, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Fvj1vnfzQU2J","colab_type":"code","colab":{}},"source":["# Lab 9 XOR\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.set_random_seed(777)  # for reproducibility\n","learning_rate = 0.01\n","\n","x_data = [[0, 0],\n","          [0, 1],\n","          [1, 0],\n","          [1, 1]]\n","y_data = [[0],\n","          [1],\n","          [1],\n","          [0]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","X = tf.placeholder(tf.float32, [None, 2], name='x-input')\n","Y = tf.placeholder(tf.float32, [None, 1], name='y-input')\n","\n","with tf.name_scope(\"layer1\"): # tensor를 묶는다\n","    W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n","    b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n","    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","\n","    w1_hist = tf.summary.histogram(\"weights1\", W1)  # logging할 값 지정\n","    b1_hist = tf.summary.histogram(\"biases1\", b1)   # logging할 값 지정\n","    layer1_hist = tf.summary.histogram(\"layer1\", layer1)   # logging할 값 지정\n","\n","\n","with tf.name_scope(\"layer2\"):   # tensor를 묶는다\n","    W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n","    b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n","    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n","\n","    w2_hist = tf.summary.histogram(\"weights2\", W2)   # logging할 값\n","    b2_hist = tf.summary.histogram(\"biases2\", b2)    # logging할 값\n","    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)  # logging할 값\n","\n","# cost/loss function\n","with tf.name_scope(\"cost\"):   # tensor를 묶는다\n","    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n","                           tf.log(1 - hypothesis))\n","    cost_summ = tf.summary.scalar(\"cost\", cost)   # logging할 값\n","\n","with tf.name_scope(\"train\"):   # tensor를 묶는다\n","    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# Accuracy computation\n","# True if hypothesis>0.5 else False\n","predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)   # logging할 값\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # tensorboard --logdir=./logs/xor_logs\n","    merged_summary = tf.summary.merge_all()   # logging할 값(tensor)들을 합한다\n","    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")   # 값을 담을 폴더 지정 /  ./logs/xor_logs_r0_01\n","    writer.add_graph(sess.graph)  # 그래프를 담는다.  Show the graph\n","\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(10001):\n","        summary, _ = sess.run([merged_summary, train], feed_dict={X: x_data, Y: y_data})   # summary를 실행한다\n","        writer.add_summary(summary, global_step=step)   # 파일로 기록\n","\n","        if step % 100 == 0:\n","            print(step, sess.run(cost, feed_dict={\n","                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n","\n","    # Accuracy report\n","    h, c, a = sess.run([hypothesis, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5biZF-2QU2N","colab_type":"code","colab":{}},"source":["# http://blog.aloni.org/posts/backprop-with-tensorflow/\n","# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.b3rvzhx89\n","# WIP\n","import tensorflow as tf\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","# tf Graph Input\n","x_data = [[1.],\n","          [2.],\n","          [3.]]\n","y_data = [[1.],\n","          [2.],\n","          [3.]]\n","\n","\n","# placeholders for a tensor that will be always fed.\n","X = tf.placeholder(tf.float32, shape=[None, 1])\n","Y = tf.placeholder(tf.float32, shape=[None, 1])\n","\n","# Set wrong model weights\n","W = tf.Variable(tf.truncated_normal([1, 1]))\n","b = tf.Variable(5.)\n","\n","# Forward prop\n","hypothesis = tf.matmul(X, W) + b\n","\n","# diff\n","assert hypothesis.shape.as_list() == Y.shape.as_list()\n","diff = (hypothesis - Y)\n","\n","# Back prop (chain rule)\n","d_l1 = diff\n","d_b = d_l1\n","d_w = tf.matmul(tf.transpose(X), d_l1)\n","\n","print(X, W, d_l1, d_w)\n","\n","# Updating network using gradients\n","learning_rate = 0.1\n","step = [\n","    tf.assign(W, W - learning_rate * d_w),\n","    tf.assign(b, b - learning_rate * tf.reduce_mean(d_b)),\n","]\n","\n","# 7. Running and testing the training process\n","RMSE = tf.reduce_mean(tf.square((Y - hypothesis)))\n","\n","sess = tf.InteractiveSession()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","\n","for i in range(1000):\n","    print(i, sess.run([step, RMSE], feed_dict={X: x_data, Y: y_data}))\n","\n","print(sess.run(hypothesis, feed_dict={X: x_data}))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MrCmaVkTQU2Q","colab_type":"code","colab":{}},"source":["\"\"\"\n","In this file, we will implement back propagations by hands\n","\n","We will use the Sigmoid Cross Entropy loss function.\n","This is equivalent to tf.nn.sigmoid_softmax_with_logits(logits, labels)\n","\n","[References]\n","\n","1) Tensorflow Document (tf.nn.sigmoid_softmax_with_logits)\n","    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n","\n","2) Neural Net Backprop in one slide! by Sung Kim\n","    https://docs.google.com/presentation/d/1_ZmtfEjLmhbuM_PqbDYMXXLAqeWN0HwuhcSKnUQZ6MM/edit#slide=id.g1ec1d04b5a_1_83\n","\n","3) Back Propagation with Tensorflow by Dan Aloni\n","    http://blog.aloni.org/posts/backprop-with-tensorflow/\n","\n","4) Yes you should understand backprop by Andrej Karpathy\n","    https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.cockptkn7\n","\n","\n","[Network Architecture]\n","\n","Input: x\n","Layer1: x * W + b\n","Output layer = σ(Layer1)\n","\n","Loss_i = - y * log(σ(Layer1)) - (1 - y) * log(1 - σ(Layer1))\n","Loss = tf.reduce_sum(Loss_i)\n","\n","We want to compute that\n","\n","dLoss/dW = ???\n","dLoss/db = ???\n","\n","please read \"Neural Net Backprop in one slide!\" for deriving formulas\n","\n","\"\"\"\n","import tensorflow as tf\n","import numpy as np\n","tf.set_random_seed(777)  # for reproducibility\n","\n","# Predicting animal type based on various features\n","xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n","X_data = xy[:, 0:-1]\n","N = X_data.shape[0]\n","y_data = xy[:, [-1]]\n","\n","# y_data has labels from 0 ~ 6\n","print(\"y has one of the following values\")\n","print(np.unique(y_data))\n","\n","# X_data.shape = (101, 16) => 101 samples, 16 features\n","# y_data.shape = (101, 1)  => 101 samples, 1 label\n","print(\"Shape of X data: \", X_data.shape)\n","print(\"Shape of y data: \", y_data.shape)\n","\n","nb_classes = 7  # 0 ~ 6\n","\n","X = tf.placeholder(tf.float32, [None, 16])\n","y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n","\n","target = tf.one_hot(y, nb_classes)  # one hot\n","target = tf.reshape(target, [-1, nb_classes])\n","target = tf.cast(target, tf.float32)\n","\n","W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n","b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n","\n","\n","def sigma(x):\n","    # sigmoid function\n","    # σ(x) = 1 / (1 + exp(-x))\n","    return 1. / (1. + tf.exp(-x))\n","\n","\n","def sigma_prime(x):\n","    # derivative of the sigmoid function\n","    # σ'(x) = σ(x) * (1 - σ(x))\n","    return sigma(x) * (1. - sigma(x))\n","\n","\n","# Forward propagtion\n","layer_1 = tf.matmul(X, W) + b\n","y_pred = sigma(layer_1)\n","\n","# Loss Function (end of forwad propagation)\n","loss_i = - target * tf.log(y_pred) - (1. - target) * tf.log(1. - y_pred)\n","loss = tf.reduce_sum(loss_i)\n","\n","# Dimension Check\n","assert y_pred.shape.as_list() == target.shape.as_list()\n","\n","\n","# Back prop (chain rule)\n","# How to derive? please read \"Neural Net Backprop in one slide!\"\n","d_loss = (y_pred - target) / (y_pred * (1. - y_pred) + 1e-7)\n","d_sigma = sigma_prime(layer_1)\n","d_layer = d_loss * d_sigma\n","d_b = d_layer\n","d_W = tf.matmul(tf.transpose(X), d_layer)\n","\n","# Updating network using gradients\n","learning_rate = 0.01\n","train_step = [\n","    tf.assign(W, W - learning_rate * d_W),\n","    tf.assign(b, b - learning_rate * tf.reduce_sum(d_b)),\n","]\n","\n","# Prediction and Accuracy\n","prediction = tf.argmax(y_pred, 1)\n","acct_mat = tf.equal(tf.argmax(y_pred, 1), tf.argmax(target, 1))\n","acct_res = tf.reduce_mean(tf.cast(acct_mat, tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(500):\n","        sess.run(train_step, feed_dict={X: X_data, y: y_data})\n","\n","        if step % 10 == 0:\n","            # Within 300 steps, you should see an accuracy of 100%\n","            step_loss, acc = sess.run([loss, acct_res], feed_dict={\n","                                      X: X_data, y: y_data})\n","            print(\"Step: {:5}\\t Loss: {:10.5f}\\t Acc: {:.2%}\" .format(\n","                step, step_loss, acc))\n","\n","    # Let's see if we can predict\n","    pred = sess.run(prediction, feed_dict={X: X_data})\n","    for p, y in zip(pred, y_data):\n","        msg = \"[{}]\\t Prediction: {:d}\\t True y: {:d}\"\n","        print(msg.format(p == int(y[0]), p, int(y[0])))\n","\n","\"\"\"\n","Output Example\n","\n","Step:     0      Loss:  453.74799        Acc: 38.61%\n","Step:    20      Loss:   95.05664        Acc: 88.12%\n","Step:    40      Loss:   66.43570        Acc: 93.07%\n","Step:    60      Loss:   53.09288        Acc: 94.06%\n","...\n","Step:   290      Loss:   18.72972        Acc: 100.00%\n","Step:   300      Loss:   18.24953        Acc: 100.00%\n","Step:   310      Loss:   17.79592        Acc: 100.00%\n","...\n","[True]   Prediction: 0   True y: 0\n","[True]   Prediction: 0   True y: 0\n","[True]   Prediction: 3   True y: 3\n","[True]   Prediction: 0   True y: 0\n","...\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"os2L3ezRQU2T","colab_type":"code","colab":{}},"source":["# Lab 9 XOR-back_prop\n","import tensorflow as tf\n","import numpy as np\n","\n","tf.set_random_seed(777)  # for reproducibility\n","learning_rate = 0.1\n","\n","x_data = [[0, 0],\n","          [0, 1],\n","          [1, 0],\n","          [1, 1]]\n","y_data = [[0],\n","          [1],\n","          [1],\n","          [0]]\n","\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","X = tf.placeholder(tf.float32, [None, 2])\n","Y = tf.placeholder(tf.float32, [None, 1])\n","\n","W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n","b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n","l1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n","\n","W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n","b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n","Y_pred = tf.sigmoid(tf.matmul(l1, W2) + b2)\n","\n","# cost/loss function\n","cost = -tf.reduce_mean(Y * tf.log(Y_pred) + (1 - Y) *\n","                       tf.log(1 - Y_pred))\n","\n","# Network\n","#          p1     a1           l1     p2     a2           l2 (y_pred)\n","# X -> (*) -> (+) -> (sigmoid) -> (*) -> (+) -> (sigmoid) -> (loss)\n","#       ^      ^                   ^      ^\n","#       |      |                   |      |\n","#       W1     b1                  W2     b2\n","\n","# Loss derivative\n","d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n","\n","# Layer 2\n","d_sigma2 = Y_pred * (1 - Y_pred)\n","d_a2 = d_Y_pred * d_sigma2\n","d_p2 = d_a2\n","d_b2 = d_a2\n","d_W2 = tf.matmul(tf.transpose(l1), d_p2)\n","\n","# Mean\n","d_b2_mean = tf.reduce_mean(d_b2, axis=[0])\n","d_W2_mean = d_W2 / tf.cast(tf.shape(l1)[0], dtype=tf.float32)\n","\n","# Layer 1\n","d_l1 = tf.matmul(d_p2, tf.transpose(W2))\n","d_sigma1 = l1 * (1 - l1)\n","d_a1 = d_l1 * d_sigma1\n","d_b1 = d_a1\n","d_p1 = d_a1\n","d_W1 = tf.matmul(tf.transpose(X), d_a1)\n","\n","# Mean\n","d_W1_mean = d_W1 / tf.cast(tf.shape(X)[0], dtype=tf.float32)\n","d_b1_mean = tf.reduce_mean(d_b1, axis=[0])\n","\n","# Weight update\n","step = [\n","  tf.assign(W2, W2 - learning_rate * d_W2_mean),\n","  tf.assign(b2, b2 - learning_rate * d_b2_mean),\n","  tf.assign(W1, W1 - learning_rate * d_W1_mean),\n","  tf.assign(b1, b1 - learning_rate * d_b1_mean)\n","]\n","\n","# Accuracy computation\n","# True if hypothesis > 0.5 else False\n","predicted = tf.cast(Y_pred > 0.5, dtype=tf.float32)\n","accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n","\n","# Launch graph\n","with tf.Session() as sess:\n","    # Initialize TensorFlow variables\n","    sess.run(tf.global_variables_initializer())\n","\n","    print(\"shape\", sess.run(tf.shape(X)[0], feed_dict={X: x_data}))\n","\n","\n","    for i in range(10001):\n","        sess.run([step, cost], feed_dict={X: x_data, Y: y_data})\n","        if i % 1000 == 0:\n","            print(i, sess.run([cost, d_W1], feed_dict={\n","                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n","\n","    # Accuracy report\n","    h, c, a = sess.run([Y_pred, predicted, accuracy],\n","                       feed_dict={X: x_data, Y: y_data})\n","    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n","\n","\n","'''\n","Hypothesis:  [[ 0.01338224]\n"," [ 0.98166382]\n"," [ 0.98809403]\n"," [ 0.01135806]]\n","Correct:  [[ 0.]\n"," [ 1.]\n"," [ 1.]\n"," [ 0.]]\n","Accuracy:  1.0\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9rRXQv3QU2W","colab_type":"code","colab":{}},"source":["# Lab 10-1 mnist_softmax - 단순한 모델, 양호한 성과\n","\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","from tensorflow.examples.tutorials.mnist import input_data\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# weights & bias for nn layers\n","W = tf.Variable(tf.random_normal([784, 10]))\n","b = tf.Variable(tf.random_normal([10]))\n","\n","hypothesis = tf.matmul(X, W) + b\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ycwElPrQU2a","colab_type":"code","colab":{}},"source":["# Lab 10 MNIST and NN - Deep Wide Learning\n","\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# weights & bias for nn layers\n","W1 = tf.Variable(tf.random_normal([784, 256]))\n","b1 = tf.Variable(tf.random_normal([256]))\n","L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n","\n","W2 = tf.Variable(tf.random_normal([256, 256]))\n","b2 = tf.Variable(tf.random_normal([256]))\n","L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","\n","W3 = tf.Variable(tf.random_normal([256, 10]))\n","b3 = tf.Variable(tf.random_normal([10]))\n","hypothesis = tf.matmul(L2, W3) + b3\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels}))\n","\n","print('Hypothesis:', sess.run(hypothesis, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels}))\n","\n","print(hypothesis.shape)\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k70LNjdWQU2g","colab_type":"code","colab":{}},"source":["# Lab 10 MNIST and Xavier - 초기부터 cost가 낮아짐\n","\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# weights & bias for nn layers\n","# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n","W1 = tf.get_variable(\"W1\", shape=[784, 256],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b1 = tf.Variable(tf.random_normal([256]))\n","L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n","\n","W2 = tf.get_variable(\"W2\", shape=[256, 256],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b2 = tf.Variable(tf.random_normal([256]))\n","L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","\n","W3 = tf.get_variable(\"W3\", shape=[256, 10],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b3 = tf.Variable(tf.random_normal([10]))\n","hypothesis = tf.matmul(L2, W3) + b3\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3GzjfrHQU2k","colab_type":"code","colab":{}},"source":["# Lab 10 MNIST and Deep learning\n","\n","\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# weights & bias for nn layers\n","# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n","W1 = tf.get_variable(\"W1\", shape=[784, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b1 = tf.Variable(tf.random_normal([512]))\n","L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n","\n","W2 = tf.get_variable(\"W2\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b2 = tf.Variable(tf.random_normal([512]))\n","L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","\n","W3 = tf.get_variable(\"W3\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b3 = tf.Variable(tf.random_normal([512]))\n","L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n","\n","W4 = tf.get_variable(\"W4\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b4 = tf.Variable(tf.random_normal([512]))\n","L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n","\n","W5 = tf.get_variable(\"W5\", shape=[512, 10],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b5 = tf.Variable(tf.random_normal([10]))\n","hypothesis = tf.matmul(L4, W5) + b5\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"48d9cOWIQU2n","colab_type":"code","colab":{}},"source":["# Lab 10 MNIST and Dropout - overfitting 해소/ AdamOptimizer\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","total_batch = int(mnist.train.num_examples / batch_size)\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n","keep_prob = tf.placeholder(tf.float32)\n","\n","# weights & bias for nn layers\n","# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n","W1 = tf.get_variable(\"W1\", shape=[784, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b1 = tf.Variable(tf.random_normal([512]))\n","L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n","L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n","\n","W2 = tf.get_variable(\"W2\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b2 = tf.Variable(tf.random_normal([512]))\n","L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n","L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n","\n","W3 = tf.get_variable(\"W3\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b3 = tf.Variable(tf.random_normal([512]))\n","L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n","L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n","\n","W4 = tf.get_variable(\"W4\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b4 = tf.Variable(tf.random_normal([512]))\n","L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n","L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n","\n","W5 = tf.get_variable(\"W5\", shape=[512, 10],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b5 = tf.Variable(tf.random_normal([10]))\n","hypothesis = tf.matmul(L4, W5) + b5\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pclk4NaBQU2r","colab_type":"code","colab":{}},"source":["# Lab 10 MNIST and High-level TF API\n","from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n","from tensorflow.contrib.framework import arg_scope\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# parameters\n","learning_rate = 0.01  # we can use large learning rate using Batch Normalization\n","training_epochs = 15\n","batch_size = 100\n","keep_prob = 0.7\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","train_mode = tf.placeholder(tf.bool, name='train_mode')\n","\n","# layer output size\n","hidden_output_size = 512\n","final_output_size = 10\n","\n","xavier_init = tf.contrib.layers.xavier_initializer()\n","bn_params = {\n","    'is_training': train_mode,\n","    'decay': 0.9,\n","    'updates_collections': None\n","}\n","\n","# We can build short code using 'arg_scope' to avoid duplicate code\n","# same function with different arguments\n","with arg_scope([fully_connected],\n","               activation_fn=tf.nn.relu,\n","               weights_initializer=xavier_init,\n","               biases_initializer=None,\n","               normalizer_fn=batch_norm,\n","               normalizer_params=bn_params\n","               ):\n","    hidden_layer1 = fully_connected(X, hidden_output_size, scope=\"h1\")\n","    h1_drop = dropout(hidden_layer1, keep_prob, is_training=train_mode)\n","    hidden_layer2 = fully_connected(h1_drop, hidden_output_size, scope=\"h2\")\n","    h2_drop = dropout(hidden_layer2, keep_prob, is_training=train_mode)\n","    hidden_layer3 = fully_connected(h2_drop, hidden_output_size, scope=\"h3\")\n","    h3_drop = dropout(hidden_layer3, keep_prob, is_training=train_mode)\n","    hidden_layer4 = fully_connected(h3_drop, hidden_output_size, scope=\"h4\")\n","    h4_drop = dropout(hidden_layer4, keep_prob, is_training=train_mode)\n","    hypothesis = fully_connected(h4_drop, final_output_size, activation_fn=None, scope=\"hypothesis\")\n","\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict_train = {X: batch_xs, Y: batch_ys, train_mode: True}\n","        feed_dict_cost = {X: batch_xs, Y: batch_ys, train_mode: False}\n","        opt = sess.run(optimizer, feed_dict=feed_dict_train)\n","        c = sess.run(cost, feed_dict=feed_dict_cost)\n","        avg_cost += c / total_batch\n","\n","    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost))\n","    #print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels, train_mode: False}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], train_mode: False}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSGNEN8QQU2u","colab_type":"code","colab":{}},"source":["# Lab 10 MNIST and Dropout\n","# SELU implementation from https://github.com/bioinf-jku/SNNs/blob/master/selu.py\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","# -*- coding: utf-8 -*-\n","'''\n","Tensorflow Implementation of the Scaled ELU function and Dropout\n","'''\n","\n","import numbers\n","from tensorflow.contrib import layers\n","from tensorflow.python.framework import ops\n","from tensorflow.python.framework import tensor_shape\n","from tensorflow.python.framework import tensor_util\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.ops import random_ops\n","from tensorflow.python.ops import array_ops\n","from tensorflow.python.layers import utils\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","def selu(x):\n","    with ops.name_scope('elu') as scope:\n","        alpha = 1.6732632423543772848170429916717\n","        scale = 1.0507009873554804934193349852946\n","        return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n","\n","\n","def dropout_selu(x, keep_prob, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0,\n","                 noise_shape=None, seed=None, name=None, training=False):\n","    \"\"\"Dropout to a value with rescaling.\"\"\"\n","\n","    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n","        keep_prob = 1.0 - rate\n","        x = ops.convert_to_tensor(x, name=\"x\")\n","        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n","            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n","                                             \"range (0, 1], got %g\" % keep_prob)\n","        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n","        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n","\n","        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n","        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n","\n","        if tensor_util.constant_value(keep_prob) == 1:\n","            return x\n","\n","        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n","        random_tensor = keep_prob\n","        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n","        binary_tensor = math_ops.floor(random_tensor)\n","        ret = x * binary_tensor + alpha * (1-binary_tensor)\n","\n","        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n","\n","        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n","        ret = a * ret + b\n","        ret.set_shape(x.get_shape())\n","        return ret\n","\n","    with ops.name_scope(name, \"dropout\", [x]) as name:\n","        return utils.smart_cond(training,\n","                                lambda: dropout_selu_impl(x, keep_prob, alpha, noise_shape, seed, name),\n","                                lambda: array_ops.identity(x))\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# parameters\n","learning_rate = 0.001\n","training_epochs = 50\n","batch_size = 100\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n","keep_prob = tf.placeholder(tf.float32)\n","\n","# weights & bias for nn layers\n","# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n","W1 = tf.get_variable(\"W1\", shape=[784, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b1 = tf.Variable(tf.random_normal([512]))\n","L1 = selu(tf.matmul(X, W1) + b1)\n","L1 = dropout_selu(L1, keep_prob=keep_prob)\n","\n","W2 = tf.get_variable(\"W2\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b2 = tf.Variable(tf.random_normal([512]))\n","L2 = selu(tf.matmul(L1, W2) + b2)\n","L2 = dropout_selu(L2, keep_prob=keep_prob)\n","\n","W3 = tf.get_variable(\"W3\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b3 = tf.Variable(tf.random_normal([512]))\n","L3 = selu(tf.matmul(L2, W3) + b3)\n","L3 = dropout_selu(L3, keep_prob=keep_prob)\n","\n","W4 = tf.get_variable(\"W4\", shape=[512, 512],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b4 = tf.Variable(tf.random_normal([512]))\n","L4 = selu(tf.matmul(L3, W4) + b4)\n","L4 = dropout_selu(L4, keep_prob=keep_prob)\n","\n","W5 = tf.get_variable(\"W5\", shape=[512, 10],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b5 = tf.Variable(tf.random_normal([10]))\n","hypothesis = tf.matmul(L4, W5) + b5\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDkP4MH9QU2z","colab_type":"code","colab":{}},"source":["# http://blog.aloni.org/posts/backprop-with-tensorflow/\n","# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.b3rvzhx89\n","import tensorflow as tf\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","\n","X = tf.placeholder(tf.float32, [None, 784])\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","w1 = tf.Variable(tf.truncated_normal([784, 30]))\n","b1 = tf.Variable(tf.truncated_normal([1, 30]))\n","w2 = tf.Variable(tf.truncated_normal([30, 10]))\n","b2 = tf.Variable(tf.truncated_normal([1, 10]))\n","\n","\n","def sigma(x):\n","    #  sigmoid function\n","    return tf.div(tf.constant(1.0),\n","                  tf.add(tf.constant(1.0), tf.exp(-x)))\n","\n","\n","def sigma_prime(x):\n","    # derivative of the sigmoid function\n","    return sigma(x) * (1 - sigma(x))\n","\n","# Forward prop\n","l1 = tf.add(tf.matmul(X, w1), b1)\n","a1 = sigma(l1)\n","l2 = tf.add(tf.matmul(a1, w2), b2)\n","y_pred = sigma(l2)\n","\n","# diff\n","assert y_pred.shape.as_list() == Y.shape.as_list()\n","diff = (y_pred - Y)\n","\n","\n","# Back prop (chain rule)\n","d_l2 = diff * sigma_prime(l2)\n","d_b2 = d_l2\n","d_w2 = tf.matmul(tf.transpose(a1), d_l2)\n","\n","d_a1 = tf.matmul(d_l2, tf.transpose(w2))\n","d_l1 = d_a1 * sigma_prime(l1)\n","d_b1 = d_l1\n","d_w1 = tf.matmul(tf.transpose(X), d_l1)\n","\n","\n","# Updating network using gradients\n","learning_rate = 0.5\n","step = [\n","    tf.assign(w1, w1 - learning_rate * d_w1),\n","    tf.assign(b1, b1 - learning_rate *\n","              tf.reduce_mean(d_b1, reduction_indices=[0])),\n","    tf.assign(w2, w2 - learning_rate * d_w2),\n","    tf.assign(b2, b2 - learning_rate *\n","              tf.reduce_mean(d_b2, reduction_indices=[0]))\n","]\n","\n","# 7. Running and testing the training process\n","acct_mat = tf.equal(tf.argmax(y_pred, 1), tf.argmax(Y, 1))\n","acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n","\n","sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())\n","\n","for i in range(10000):\n","    batch_xs, batch_ys = mnist.train.next_batch(10)\n","    sess.run(step, feed_dict={X: batch_xs,\n","                              Y: batch_ys})\n","    if i % 1000 == 0:\n","        res = sess.run(acct_res, feed_dict={X: mnist.test.images[:1000],\n","                                            Y: mnist.test.labels[:1000]})\n","        print(res)\n","\n","# 8. Automatic differentiation in TensorFlow\n","cost = diff * diff\n","step = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhj-_P2BQU23","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TFT2s9sQU26","colab_type":"text"},"source":["## convolution"]},{"cell_type":"code","metadata":{"id":"Kr5O_P7OQU27","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_0fqeCUQU2_","colab_type":"code","colab":{}},"source":["sess = tf.InteractiveSession()\n","\n","image = np.array([[[[1], [2], [3]],\n","                [[4], [5], [6]],\n","                [[7], [8], [9]]]], dtype=np.float32)\n","print(image.shape)\n","plt.imshow(image.reshape(3,3), cmap='Greys')\n","print(image.reshape(3,3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"22QuISi5QU3C","colab_type":"code","colab":{}},"source":["print('image.shape', image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BDutScsRQU3F","colab_type":"code","colab":{}},"source":["weight = tf.constant([[[[1.]], [[1.]]],\n","                     [[[1.]], [[1.]]]])\n","print(weight.shape)\n","conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID')\n","conv2d_img = conv2d.eval()\n","print(conv2d_img.shape)\n","print(conv2d_img.reshape(2,2))\n","plt.imshow(conv2d_img.reshape(2,2), cmap='Greys')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MK9CcLxSQU3H","colab_type":"code","colab":{}},"source":["conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n","print(conv2d_img.shape)\n","for i, one_img in enumerate(conv2d_img):\n","    print(one_img.reshape(2,2))\n","    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwVp3cXeQU3O","colab_type":"code","colab":{}},"source":["## 3 filters"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgCppGrdQU3Q","colab_type":"code","colab":{}},"source":["weight = tf.constant([[[[1.,10.,-1.]], [[1., 10., -1.]]],\n","                     [[[1.,10.,-1.]], [[1., 10., -1.]]]])\n","print(weight.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gw-XGNKDQU3T","colab_type":"code","colab":{}},"source":["conv2d = tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding='SAME')\n","conv2d_img = conv2d.eval()\n","print(conv2d_img.shape)\n","conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n","for i, one_img in enumerate(conv2d_img):\n","    print(one_img.reshape(3,3))\n","    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7FyzOyfQU3W","colab_type":"code","colab":{}},"source":["print(image)\n","print(image.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1m1OZ6SQU3Z","colab_type":"code","colab":{}},"source":["print(image.swapaxes(0,2).shape)\n","print(image.swapaxes(0,2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UITdonHTQU3b","colab_type":"code","colab":{}},"source":["print(image.swapaxes(2,3).shape)\n","print(image.swapaxes(2,3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RvTLpFhSQU3e","colab_type":"code","colab":{}},"source":["image = np.array([[[[4], [3]],\n","                  [[2], [1]]]], dtype=np.float32)\n","pool = tf.nn.max_pool(image, ksize=[1,2,2,1], strides=[1,1,1,1], padding='SAME')\n","print(pool.shape)\n","print(pool.eval())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UwiIeopQU3h","colab_type":"code","colab":{}},"source":["from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets('../../MNIST_data/', one_hot=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WX_aUcvMQU3k","colab_type":"code","colab":{}},"source":["img = mnist.train.images[0].reshape(28,28)\n","plt.imshow(img, cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQzd2EFcQU3n","colab_type":"code","colab":{}},"source":["sess = tf.InteractiveSession()\n","img = img.reshape(-1, 28, 28, 1)\n","w1 = tf.Variable(tf.random_normal([3,3,1,5], stddev = 0.01))\n","conv2d = tf.nn.conv2d(img, w1, strides=[1,2,2,1], padding='SAME')\n","print(conv2d)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qWVLzk9QU3q","colab_type":"code","colab":{}},"source":["sess.run(tf.global_variables_initializer())\n","conv2d_img = conv2d.eval()\n","conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n","for i, one_img in enumerate(conv2d_img):\n","    plt.subplot(1, 5, 1+i), plt.imshow(one_img.reshape(14,14), cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b73TygmDQU3t","colab_type":"code","colab":{}},"source":["pool = tf.nn.max_pool(conv2d, ksize = [1,2,2,1], strides=[1,2,2,1], padding='SAME')\n","print(pool)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"huiEtDqTQU3y","colab_type":"code","colab":{}},"source":["sess.run(tf.global_variables_initializer())\n","pool_img = pool.eval()\n","pool_img = np.swapaxes(pool_img, 0, 3)\n","for i, one_img in enumerate(pool_img):\n","    plt.subplot(1,5,1+i), plt.imshow(one_img.reshape(7,7), cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dkTSDIKQU34","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfY5VGKgQU37","colab_type":"code","colab":{}},"source":["# Lab 11 MNIST and Convolutional Neural Network\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"../../MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# hyper parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# L1 ImgIn shape=(?, 28, 28, 1)\n","W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n","#    Conv     -> (?, 28, 28, 32)\n","#    Pool     -> (?, 14, 14, 32)\n","L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n","L1 = tf.nn.relu(L1)\n","L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n","                    strides=[1, 2, 2, 1], padding='SAME')\n","'''\n","Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n","Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n","Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n","'''\n","\n","# L2 ImgIn shape=(?, 14, 14, 32)\n","W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n","#    Conv      ->(?, 14, 14, 64)\n","#    Pool      ->(?, 7, 7, 64)\n","L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n","L2 = tf.nn.relu(L2)\n","L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n","                    strides=[1, 2, 2, 1], padding='SAME')\n","L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n","'''\n","Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n","Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n","'''\n","\n","# Final FC 7x7x64 inputs -> 10 outputs\n","W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b = tf.Variable(tf.random_normal([10]))\n","logits = tf.matmul(L2_flat, W3) + b\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=logits, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","print('Learning started. It takes sometime.')\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()\n","\n","\n","'''\n","Epoch: 0001 cost = 0.340291267\n","Epoch: 0002 cost = 0.090731326\n","Epoch: 0003 cost = 0.064477619\n","Epoch: 0004 cost = 0.050683064\n","Epoch: 0005 cost = 0.041864835\n","Epoch: 0006 cost = 0.035760704\n","Epoch: 0007 cost = 0.030572132\n","Epoch: 0008 cost = 0.026207981\n","Epoch: 0009 cost = 0.022622454\n","Epoch: 0010 cost = 0.019055919\n","Epoch: 0011 cost = 0.017758641\n","Epoch: 0012 cost = 0.014156652\n","Epoch: 0013 cost = 0.012397016\n","Epoch: 0014 cost = 0.010693789\n","Epoch: 0015 cost = 0.009469977\n","Learning Finished!\n","Accuracy: 0.9885\n","'''\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1OndGHDQU3-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdI5aKuDQU4A","colab_type":"code","colab":{}},"source":["# Lab 11 MNIST and Deep learning CNN\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"../../MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# hyper parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n","keep_prob = tf.placeholder(tf.float32)\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# L1 ImgIn shape=(?, 28, 28, 1)\n","W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n","#    Conv     -> (?, 28, 28, 32)\n","#    Pool     -> (?, 14, 14, 32)\n","L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n","L1 = tf.nn.relu(L1)\n","L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n","                    strides=[1, 2, 2, 1], padding='SAME')\n","L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n","'''\n","Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n","Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n","Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n","Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n","'''\n","\n","# L2 ImgIn shape=(?, 14, 14, 32)\n","W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n","#    Conv      ->(?, 14, 14, 64)\n","#    Pool      ->(?, 7, 7, 64)\n","L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n","L2 = tf.nn.relu(L2)\n","L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n","                    strides=[1, 2, 2, 1], padding='SAME')\n","L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n","'''\n","Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n","Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n","'''\n","\n","# L3 ImgIn shape=(?, 7, 7, 64)\n","W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n","#    Conv      ->(?, 7, 7, 128)\n","#    Pool      ->(?, 4, 4, 128)\n","#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n","L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n","L3 = tf.nn.relu(L3)\n","L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n","                    1, 2, 2, 1], padding='SAME')\n","L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n","L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n","'''\n","Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n","Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n","Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n","'''\n","\n","# L4 FC 4x4x128 inputs -> 625 outputs\n","W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b4 = tf.Variable(tf.random_normal([625]))\n","L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n","L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n","'''\n","Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n","Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n","'''\n","\n","# L5 Final FC 625 inputs -> 10 outputs\n","W5 = tf.get_variable(\"W5\", shape=[625, 10],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b5 = tf.Variable(tf.random_normal([10]))\n","logits = tf.matmul(L4, W5) + b5\n","'''\n","Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n","'''\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=logits, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","print('Learning started. It takes sometime.')\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n","        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","\n","# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n","\n","correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","print('Accuracy:', sess.run(accuracy, feed_dict={\n","      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n","\n","# Get one and predict\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()\n","\n","'''\n","Learning stared. It takes sometime.\n","Epoch: 0001 cost = 0.385748474\n","Epoch: 0002 cost = 0.092017397\n","Epoch: 0003 cost = 0.065854684\n","Epoch: 0004 cost = 0.055604566\n","Epoch: 0005 cost = 0.045996377\n","Epoch: 0006 cost = 0.040913645\n","Epoch: 0007 cost = 0.036924479\n","Epoch: 0008 cost = 0.032808939\n","Epoch: 0009 cost = 0.031791007\n","Epoch: 0010 cost = 0.030224456\n","Epoch: 0011 cost = 0.026849916\n","Epoch: 0012 cost = 0.026826763\n","Epoch: 0013 cost = 0.027188021\n","Epoch: 0014 cost = 0.023604777\n","Epoch: 0015 cost = 0.024607201\n","Learning Finished!\n","Accuracy: 0.9938\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPwbA4HxQU4C","colab_type":"code","colab":{}},"source":["# Lab 11 MNIST and Deep learning CNN - Class\n","import tensorflow as tf\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# hyper parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","class Model:\n","\n","    def __init__(self, sess, name):\n","        self.sess = sess\n","        self.name = name\n","        self._build_net()\n","\n","    def _build_net(self):\n","        with tf.variable_scope(self.name):\n","            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n","            # for testing\n","            self.keep_prob = tf.placeholder(tf.float32)\n","\n","            # input place holders\n","            self.X = tf.placeholder(tf.float32, [None, 784])\n","            # img 28x28x1 (black/white)\n","            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n","            self.Y = tf.placeholder(tf.float32, [None, 10])\n","\n","            # L1 ImgIn shape=(?, 28, 28, 1)\n","            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n","            #    Conv     -> (?, 28, 28, 32)\n","            #    Pool     -> (?, 14, 14, 32)\n","            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n","            L1 = tf.nn.relu(L1)\n","            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n","                                strides=[1, 2, 2, 1], padding='SAME')\n","            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n","            '''\n","            Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n","            Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n","            Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n","            Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n","            '''\n","\n","            # L2 ImgIn shape=(?, 14, 14, 32)\n","            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n","            #    Conv      ->(?, 14, 14, 64)\n","            #    Pool      ->(?, 7, 7, 64)\n","            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n","            L2 = tf.nn.relu(L2)\n","            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n","                                strides=[1, 2, 2, 1], padding='SAME')\n","            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n","            '''\n","            Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","            Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","            Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n","            Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n","            '''\n","\n","            # L3 ImgIn shape=(?, 7, 7, 64)\n","            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n","            #    Conv      ->(?, 7, 7, 128)\n","            #    Pool      ->(?, 4, 4, 128)\n","            #    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n","            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n","            L3 = tf.nn.relu(L3)\n","            L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n","                                1, 2, 2, 1], padding='SAME')\n","            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n","\n","            L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n","            '''\n","            Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","            Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","            Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n","            Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n","            Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n","            '''\n","\n","            # L4 FC 4x4x128 inputs -> 625 outputs\n","            W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n","                                 initializer=tf.contrib.layers.xavier_initializer())\n","            b4 = tf.Variable(tf.random_normal([625]))\n","            L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n","            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n","            '''\n","            Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n","            Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n","            '''\n","\n","            # L5 Final FC 625 inputs -> 10 outputs\n","            W5 = tf.get_variable(\"W5\", shape=[625, 10],\n","                                 initializer=tf.contrib.layers.xavier_initializer())\n","            b5 = tf.Variable(tf.random_normal([10]))\n","            self.logits = tf.matmul(L4, W5) + b5\n","            '''\n","            Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n","            '''\n","\n","        # define cost/loss & optimizer\n","        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","            logits=self.logits, labels=self.Y))\n","        self.optimizer = tf.train.AdamOptimizer(\n","            learning_rate=learning_rate).minimize(self.cost)\n","\n","        correct_prediction = tf.equal(\n","            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n","        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","    def predict(self, x_test, keep_prop=1.0):\n","        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.keep_prob: keep_prop})\n","\n","    def get_accuracy(self, x_test, y_test, keep_prop=1.0):\n","        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prop})\n","\n","    def train(self, x_data, y_data, keep_prop=0.7):\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={\n","            self.X: x_data, self.Y: y_data, self.keep_prob: keep_prop})\n","\n","# initialize\n","sess = tf.Session()\n","m1 = Model(sess, \"m1\")\n","\n","sess.run(tf.global_variables_initializer())\n","\n","print('Learning Started!')\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        c, _ = m1.train(batch_xs, batch_ys)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","print('Accuracy:', m1.get_accuracy(mnist.test.images, mnist.test.labels))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QK1MyNvQU4F","colab_type":"code","colab":{}},"source":["# Lab 11 MNIST and Deep learning CNN - ensemble\n","# https://www.tensorflow.org/tutorials/layers\n","import tensorflow as tf\n","import numpy as np\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"../../MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# hyper parameters\n","learning_rate = 0.001\n","training_epochs = 20\n","batch_size = 100\n","\n","\n","class Model:\n","\n","    def __init__(self, sess, name):\n","        self.sess = sess\n","        self.name = name\n","        self._build_net()\n","\n","    def _build_net(self):\n","        with tf.variable_scope(self.name):\n","            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n","            # for testing\n","            self.training = tf.placeholder(tf.bool)\n","\n","            # input place holders\n","            self.X = tf.placeholder(tf.float32, [None, 784])\n","\n","            # img 28x28x1 (black/white), Input Layer\n","            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n","            self.Y = tf.placeholder(tf.float32, [None, 10])\n","\n","            # Convolutional Layer #1\n","            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n","                                     padding=\"SAME\", activation=tf.nn.relu)\n","            # Pooling Layer #1\n","            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n","                                            padding=\"SAME\", strides=2)\n","            dropout1 = tf.layers.dropout(inputs=pool1,\n","                                         rate=0.3, training=self.training)\n","\n","            # Convolutional Layer #2 and Pooling Layer #2\n","            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n","                                     padding=\"SAME\", activation=tf.nn.relu)\n","            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n","                                            padding=\"SAME\", strides=2)\n","            dropout2 = tf.layers.dropout(inputs=pool2,\n","                                         rate=0.3, training=self.training)\n","\n","            # Convolutional Layer #3 and Pooling Layer #3\n","            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n","                                     padding=\"SAME\", activation=tf.nn.relu)\n","            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n","                                            padding=\"SAME\", strides=2)\n","            dropout3 = tf.layers.dropout(inputs=pool3,\n","                                         rate=0.3, training=self.training)\n","\n","            # Dense Layer with Relu\n","            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n","            dense4 = tf.layers.dense(inputs=flat,\n","                                     units=625, activation=tf.nn.relu)\n","            dropout4 = tf.layers.dropout(inputs=dense4,\n","                                         rate=0.5, training=self.training)\n","\n","            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n","            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n","\n","        # define cost/loss & optimizer\n","        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","            logits=self.logits, labels=self.Y))\n","        self.optimizer = tf.train.AdamOptimizer(\n","            learning_rate=learning_rate).minimize(self.cost)\n","\n","        correct_prediction = tf.equal(\n","            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n","        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","    def predict(self, x_test, training=False):\n","        return self.sess.run(self.logits,\n","                             feed_dict={self.X: x_test, self.training: training})\n","\n","    def get_accuracy(self, x_test, y_test, training=False):\n","        return self.sess.run(self.accuracy,\n","                             feed_dict={self.X: x_test,\n","                                        self.Y: y_test, self.training: training})\n","\n","    def train(self, x_data, y_data, training=True):\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={\n","            self.X: x_data, self.Y: y_data, self.training: training})\n","\n","# initialize\n","sess = tf.Session()\n","\n","models = []\n","num_models = 2\n","for m in range(num_models):\n","    models.append(Model(sess, \"model\" + str(m)))\n","\n","sess.run(tf.global_variables_initializer())\n","\n","print('Learning Started!')\n","\n","# train my model\n","for epoch in range(training_epochs):\n","    avg_cost_list = np.zeros(len(models))\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","\n","        # train each model\n","        for m_idx, m in enumerate(models):\n","            c, _ = m.train(batch_xs, batch_ys)\n","            avg_cost_list[m_idx] += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","test_size = len(mnist.test.labels)\n","predictions = np.zeros([test_size, 10])\n","for m_idx, m in enumerate(models):\n","    print(m_idx, 'Accuracy:', m.get_accuracy(\n","        mnist.test.images, mnist.test.labels))\n","    p = m.predict(mnist.test.images)\n","    predictions += p\n","\n","ensemble_correct_prediction = tf.equal(\n","    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n","ensemble_accuracy = tf.reduce_mean(\n","    tf.cast(ensemble_correct_prediction, tf.float32))\n","print('Ensemble accuracy:', sess.run(ensemble_accuracy))\n","\n","'''\n","0 Accuracy: 0.9933\n","1 Accuracy: 0.9946\n","2 Accuracy: 0.9934\n","3 Accuracy: 0.9935\n","4 Accuracy: 0.9935\n","5 Accuracy: 0.9949\n","6 Accuracy: 0.9941\n","\n","Ensemble accuracy: 0.9952\n","'''\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9bR0lLPQU4H","colab_type":"code","colab":{}},"source":["# Lab 10 MNIST and Deep learning CNN - low memory\n","import tensorflow as tf\n","import random\n","# import matplotlib.pyplot as plt\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","tf.set_random_seed(777)  # reproducibility\n","\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n","# more information about the mnist dataset\n","\n","# hyper parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100\n","\n","# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n","keep_prob = tf.placeholder(tf.float32)\n","\n","# input place holders\n","X = tf.placeholder(tf.float32, [None, 784])\n","X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n","Y = tf.placeholder(tf.float32, [None, 10])\n","\n","# L1 ImgIn shape=(?, 28, 28, 1)\n","W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n","#    Conv     -> (?, 28, 28, 32)\n","#    Pool     -> (?, 14, 14, 32)\n","L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n","L1 = tf.nn.relu(L1)\n","L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n","                    strides=[1, 2, 2, 1], padding='SAME')\n","L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n","'''\n","Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n","Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n","Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n","Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n","'''\n","\n","# L2 ImgIn shape=(?, 14, 14, 32)\n","W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n","#    Conv      ->(?, 14, 14, 64)\n","#    Pool      ->(?, 7, 7, 64)\n","L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n","L2 = tf.nn.relu(L2)\n","L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n","                    strides=[1, 2, 2, 1], padding='SAME')\n","L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n","'''\n","Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n","Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n","'''\n","\n","# L3 ImgIn shape=(?, 7, 7, 64)\n","W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n","#    Conv      ->(?, 7, 7, 128)\n","#    Pool      ->(?, 4, 4, 128)\n","#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n","L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n","L3 = tf.nn.relu(L3)\n","L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n","                    1, 2, 2, 1], padding='SAME')\n","L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n","L3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n","'''\n","Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n","Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n","Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n","'''\n","\n","# L4 FC 4x4x128 inputs -> 625 outputs\n","W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b4 = tf.Variable(tf.random_normal([625]))\n","L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n","L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n","'''\n","Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n","Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n","'''\n","\n","# L5 Final FC 625 inputs -> 10 outputs\n","W5 = tf.get_variable(\"W5\", shape=[625, 10],\n","                     initializer=tf.contrib.layers.xavier_initializer())\n","b5 = tf.Variable(tf.random_normal([10]))\n","hypothesis = tf.matmul(L4, W5) + b5\n","'''\n","Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n","'''\n","\n","# define cost/loss & optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","    logits=hypothesis, labels=Y))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","\n","# initialize\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# train my model\n","print('Learning stared. It takes sometime.')\n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    total_batch = int(mnist.train.num_examples / batch_size)\n","\n","    for i in range(total_batch):\n","        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n","        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n","        avg_cost += c / total_batch\n","\n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning Finished!')\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","\n","def evaluate(X_sample, y_sample, batch_size=512):\n","    \"\"\"Run a minibatch accuracy op\"\"\"\n","\n","    N = X_sample.shape[0]\n","    correct_sample = 0\n","\n","    for i in range(0, N, batch_size):\n","        X_batch = X_sample[i: i + batch_size]\n","        y_batch = y_sample[i: i + batch_size]\n","        N_batch = X_batch.shape[0]\n","\n","        feed = {\n","            X: X_batch,\n","            Y: y_batch,\n","            keep_prob: 1\n","        }\n","\n","        correct_sample += sess.run(accuracy, feed_dict=feed) * N_batch\n","\n","    return correct_sample / N\n","\n","print(\"\\nAccuracy Evaluates\")\n","print(\"-------------------------------\")\n","print('Train Accuracy:', evaluate(mnist.train.images, mnist.train.labels))\n","print('Test Accuracy:', evaluate(mnist.test.images, mnist.test.labels))\n","\n","\n","# Get one and predict\n","print(\"\\nGet one and predict\")\n","print(\"-------------------------------\")\n","r = random.randint(0, mnist.test.num_examples - 1)\n","print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n","print(\"Prediction: \", sess.run(\n","    tf.argmax(hypothesis, 1), {X: mnist.test.images[r:r + 1], keep_prob: 1}))\n","\n","# plt.imshow(mnist.test.images[r:r + 1].\n","#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n","# plt.show()\n","\n","'''\n","Learning stared. It takes sometime.\n","Epoch: 0001 cost = 0.385748474\n","Epoch: 0002 cost = 0.092017397\n","Epoch: 0003 cost = 0.065854684\n","Epoch: 0004 cost = 0.055604566\n","Epoch: 0005 cost = 0.045996377\n","Epoch: 0006 cost = 0.040913645\n","Epoch: 0007 cost = 0.036924479\n","Epoch: 0008 cost = 0.032808939\n","Epoch: 0009 cost = 0.031791007\n","Epoch: 0010 cost = 0.030224456\n","Epoch: 0011 cost = 0.026849916\n","Epoch: 0012 cost = 0.026826763\n","Epoch: 0013 cost = 0.027188021\n","Epoch: 0014 cost = 0.023604777\n","Epoch: 0015 cost = 0.024607201\n","Learning Finished!\n","Accuracy: 0.9938\n","'''\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4fUznUYQU4J","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","hidden_size = 2\n","\n","cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size)\n","\n","x_data = np.array([[[1,0,0,0]]],dtype=np.float32)\n","outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    print(outputs.eval())\n","    print(_states.eval())\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tXIBnESQU4O","colab_type":"code","colab":{}},"source":["# Lab 12 RNN\n","import tensorflow as tf\n","import numpy as np\n","tf.set_random_seed(777)  # reproducibility\n","\n","idx2char = ['h', 'i', 'e', 'l', 'o']\n","# Teach hello: hihell -> ihello\n","x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n","x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n","              [0, 1, 0, 0, 0],   # i 1\n","              [1, 0, 0, 0, 0],   # h 0\n","              [0, 0, 1, 0, 0],   # e 2\n","              [0, 0, 0, 1, 0],   # l 3\n","              [0, 0, 0, 1, 0]]]  # l 3\n","\n","y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n","\n","num_classes = 5\n","input_dim = 5  # one-hot size\n","hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n","batch_size = 1   # one sentence\n","sequence_length = 6  # |ihello| == 6\n","learning_rate = 0.1\n","\n","X = tf.placeholder(\n","    tf.float32, [None, sequence_length, input_dim])  # X one-hot\n","Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n","\n","cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n","initial_state = cell.zero_state(batch_size, tf.float32)\n","outputs, _states = tf.nn.dynamic_rnn(\n","    cell, X, initial_state=initial_state, dtype=tf.float32)\n","\n","# FC layer\n","X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n","# fc_w = tf.get_variable(\"fc_w\", [hidden_size, num_classes])\n","# fc_b = tf.get_variable(\"fc_b\", [num_classes])\n","# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\n","outputs = tf.contrib.layers.fully_connected(\n","    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n","\n","# reshape out for sequence_loss\n","outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n","\n","weights = tf.ones([batch_size, sequence_length])\n","sequence_loss = tf.contrib.seq2seq.sequence_loss(\n","    logits=outputs, targets=Y, weights=weights)\n","loss = tf.reduce_mean(sequence_loss)\n","train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n","\n","prediction = tf.argmax(outputs, axis=2)\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    for i in range(50):\n","        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n","        result = sess.run(prediction, feed_dict={X: x_one_hot})\n","        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n","\n","        # print char using dic\n","        result_str = [idx2char[c] for c in np.squeeze(result)]\n","        print(\"\\tPrediction str: \", ''.join(result_str))\n","\n","'''\n","0 loss: 1.71584 prediction:  [[2 2 2 3 3 2]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  eeelle\n","1 loss: 1.56447 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  llllll\n","2 loss: 1.46284 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  llllll\n","3 loss: 1.38073 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  llllll\n","4 loss: 1.30603 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  llllll\n","5 loss: 1.21498 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  llllll\n","6 loss: 1.1029 prediction:  [[3 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  lhlllo\n","7 loss: 0.982386 prediction:  [[1 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  ihlllo\n","8 loss: 0.871259 prediction:  [[1 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  ihlllo\n","9 loss: 0.774338 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  ihello\n","10 loss: 0.676005 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n","\tPrediction str:  ihello\n","\n","...\n","\n","'''\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDWfD6aSQU4Q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iB0o14nSQU4T","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JeQsz0PQU4Y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IOwKGcLQU4a","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hp9EiGuOQU4c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpGgkxCxQU4e","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHzz4fxKQU4g","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Gg-SaF-QU4m","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6a7UsTczQU4o","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhMwpIMJQU4r","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"baV-NcU4QU4t","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ieQLv-DTQU4v","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SK3LnbCYQU4y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJh4leFWQU41","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3PEq4mqQU44","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"53bblbt3QU46","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LGDUKkEQU49","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MEqk9m4QU4_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"epL1mfd6QU5J","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQHGGZe-QU5O","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DoLETzBQU5Q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHXPcPdXQU5T","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPzFgpwGQU5V","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTYaru6VQU5X","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvIBrG51QU5Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tA1SPm9AQU5b","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiSgfTdDQU5d","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t22u8JZFQU5g","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}